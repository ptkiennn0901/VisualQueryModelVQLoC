{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T11:03:43.581460Z",
     "iopub.status.busy": "2025-11-05T11:03:43.581189Z",
     "iopub.status.idle": "2025-11-05T11:03:43.585929Z",
     "shell.execute_reply": "2025-11-05T11:03:43.585127Z",
     "shell.execute_reply.started": "2025-11-05T11:03:43.581438Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/hwjiang1510/VQLoC.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T11:03:43.587127Z",
     "iopub.status.busy": "2025-11-05T11:03:43.586967Z",
     "iopub.status.idle": "2025-11-05T11:03:43.605989Z",
     "shell.execute_reply": "2025-11-05T11:03:43.605443Z",
     "shell.execute_reply.started": "2025-11-05T11:03:43.587114Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# os.chdir('/kaggle/working/VQLoC')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T11:05:53.911382Z",
     "iopub.status.busy": "2025-11-05T11:05:53.911100Z",
     "iopub.status.idle": "2025-11-05T11:05:56.921618Z",
     "shell.execute_reply": "2025-11-05T11:05:56.920986Z",
     "shell.execute_reply.started": "2025-11-05T11:05:53.911357Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import itertools\n",
    "import argparse\n",
    "import kornia\n",
    "import kornia.augmentation as K\n",
    "from kornia.constants import DataKey\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "NORMALIZE_MEAN = [0.485, 0.456, 0.406]\n",
    "NORMALIZE_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "def process_data(config, sample, iter=0, split='train', device='cuda'):\n",
    "    '''\n",
    "    sample: \n",
    "        'clip': clip,                           # [B,T,3,H,W]\n",
    "        'clip_with_bbox': clip_with_bbox,       # [B,T], binary value 0 / 1\n",
    "        'clip_bbox': clip_bbox,                 # [B,T,4]\n",
    "        'query': query                          # [B,3,H2,W2]\n",
    "    '''    \n",
    "    B, T, _, H, W = sample['clip'].shape\n",
    "    B, _, H2, W2 = sample['query'].shape\n",
    "    normalization = kornia.enhance.Normalize(mean=NORMALIZE_MEAN, std=NORMALIZE_STD)\n",
    "\n",
    "    brightness = config.train.aug_brightness\n",
    "    contrast = config.train.aug_contrast\n",
    "    saturation = config.train.aug_saturation\n",
    "    query_size = config.dataset.query_size\n",
    "    crop_sacle = config.train.aug_crop_scale\n",
    "    crop_ratio_min = config.train.aug_crop_ratio_min\n",
    "    crop_ratio_max = config.train.aug_crop_ratio_max\n",
    "    affine_degree = config.train.aug_affine_degree\n",
    "    affine_translate = config.train.aug_affine_translate\n",
    "    affine_scale_min = config.train.aug_affine_scale_min\n",
    "    affine_scale_max = config.train.aug_affine_scale_max\n",
    "    affine_shear_min = config.train.aug_affine_shear_min\n",
    "    affine_shear_max = config.train.aug_affine_shear_max\n",
    "    prob_color = config.train.aug_prob_color\n",
    "    prob_flip = config.train.aug_prob_flip\n",
    "    prob_crop = config.train.aug_prob_crop\n",
    "    prob_affine = config.train.aug_prob_affine\n",
    "\n",
    "    transform_clip = K.AugmentationSequential(\n",
    "                K.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.3, hue=0, p=1.0),\n",
    "                K.RandomHorizontalFlip(p=0.5),\n",
    "                K.RandomResizedCrop((H, W), scale=(0.66, 1.0), ratio=(crop_ratio_min, crop_ratio_max), p=1.0),\n",
    "                # K.RandomAffine(affine_degree, [affine_translate, affine_translate], [affine_scale_min, affine_scale_max], \n",
    "                #                 [affine_shear_min, affine_shear_max], p=prob_affine),\n",
    "                data_keys=[DataKey.INPUT, DataKey.BBOX_XYXY],  # Just to define the future input here.\n",
    "                same_on_batch=True,\n",
    "                )\n",
    "    transform_query = K.AugmentationSequential(\n",
    "                K.ColorJitter(brightness, contrast, saturation, hue=0, p=prob_color),\n",
    "                K.RandomHorizontalFlip(p=prob_flip),\n",
    "                K.RandomResizedCrop((query_size, query_size), scale=(crop_sacle, 1.0), ratio=(crop_ratio_min, crop_ratio_max), p=prob_crop),\n",
    "                # K.RandomAffine(affine_degree, [affine_translate, affine_translate], [affine_scale_min, affine_scale_max], \n",
    "                #                 [affine_shear_min, affine_shear_max], p=prob_affine\n",
    "                # K.RandomAffine(affine_degree, [0, 0], [1.0, 1.0], \n",
    "                #                 [1.0, 1.0], p=prob_affine),\n",
    "                data_keys=[\"input\"],  # Just to define the future input here.\n",
    "                same_on_batch=False,\n",
    "                )\n",
    "    \n",
    "    transform_query_frame = K.AugmentationSequential(\n",
    "                K.ColorJitter(brightness, contrast, saturation, hue=0, p=prob_color),\n",
    "                K.RandomHorizontalFlip(p=prob_flip),\n",
    "                # K.RandomAffine(affine_degree, [affine_translate, affine_translate], [affine_scale_min, affine_scale_max], \n",
    "                #                 [affine_shear_min, affine_shear_max], p=prob_affine\n",
    "                # K.RandomAffine(affine_degree, [0, 0], [1.0, 1.0], \n",
    "                #                 [1.0, 1.0], p=prob_affine),\n",
    "                data_keys=[DataKey.INPUT, DataKey.BBOX_XYXY],  # Just to define the future input here.\n",
    "                same_on_batch=False,\n",
    "                )\n",
    "    \n",
    "    clip = sample['clip']                           # [B,T,C,H,W]\n",
    "    query = sample['query']                         # [B,C,H',W']\n",
    "    clip_with_bbox = sample['clip_with_bbox']       # [B,T]\n",
    "    clip_bbox = sample['clip_bbox']                 # [B,T,4], with value range [0,1], torch axis\n",
    "    clip_bbox = recover_bbox(clip_bbox, H, W)       # [B,T,4], with range in image pixels, torch axis\n",
    "    clip_bbox = bbox_torchTocv2(clip_bbox)          # [B,T,4], with range in image pixels, cv2 axis\n",
    "    if config.train.use_query_roi and 'query_frame' in sample.keys():\n",
    "        query_frame = sample['query_frame']                         # [B,C,H,W]\n",
    "        query_frame_bbox = sample['query_frame_bbox']   \n",
    "        query_frame_bbox = recover_bbox(query_frame_bbox, H, W)\n",
    "        query_frame_bbox = bbox_torchTocv2(query_frame_bbox)        # [B,4]\n",
    "\n",
    "    # augment clips\n",
    "    if split == 'train' and config.train.aug_clip and (iter > config.train.aug_clip_iter):        \n",
    "        clip_aug, clip_bbox_aug  = [], []\n",
    "        for clip_cur, clip_bbox_cur in zip(clip, clip_bbox):    # [T,C,H,W], [T,4,2]\n",
    "            clip_cur_aug, clip_bbox_cur_aug = transform_clip(clip_cur.to(device), clip_bbox_cur.to(device).unsqueeze(1))\n",
    "            clip_aug.append(clip_cur_aug)\n",
    "            clip_bbox_aug.append(clip_bbox_cur_aug.squeeze())\n",
    "\n",
    "        clip_aug = torch.stack(clip_aug)                     # [B,T,C,H,W]\n",
    "        clip_bbox_aug = torch.stack(clip_bbox_aug)           # [B,T,4]\n",
    "        clip_bbox_aug = bbox_cv2Totorch(clip_bbox_aug)\n",
    "        clip_bbox_aug, with_bbox_update = check_bbox(clip_bbox_aug, H, W)\n",
    "        clip_bbox_aug = normalize_bbox(clip_bbox_aug, H, W)                 # back in range [0,1]\n",
    "        clip_with_bbox_aug = torch.logical_and(with_bbox_update.to(clip_with_bbox.device), clip_with_bbox)\n",
    "        sample['clip'] = clip_aug.to(device)\n",
    "        sample['clip_with_bbox'] = clip_with_bbox_aug.to(device).float()\n",
    "        sample['clip_bbox'] = clip_bbox_aug.to(device)\n",
    "    \n",
    "    # augment the query\n",
    "    if split == 'train' and config.train.aug_query:\n",
    "        query = transform_query(query)\n",
    "        sample['query'] = query.to(device)\n",
    "    \n",
    "    # augment the query frame\n",
    "    if split == 'train' and config.train.aug_query and 'query_frame' in sample.keys():\n",
    "        query_frame, query_frame_bbox = transform_query_frame(query)\n",
    "        sample['query_frame'] = query_frame.to(device)\n",
    "        query_frame_bbox = bbox_cv2Totorch(query_frame_bbox)\n",
    "        query_frame_bbox = normalize_bbox(query_frame_bbox, H, W).clamp(min=0.0, max=1.0)\n",
    "        sample['query_frame_bbox'] = query_frame_bbox.to(device).float()\n",
    "\n",
    "    # normalize the input clips\n",
    "    sample['clip_origin'] = sample['clip'].clone()\n",
    "    clip = rearrange(sample['clip'], 'b t c h w -> (b t) c h w').to(device)\n",
    "    clip = normalization(clip)\n",
    "    sample['clip'] = rearrange(clip, '(b t) c h w -> b t c h w', b=B, t=T)\n",
    "\n",
    "    # normalize input query\n",
    "    sample['query_origin'] = sample['query'].clone()\n",
    "    sample['query'] = normalization(sample['query'])\n",
    "\n",
    "    # normalize input query frame\n",
    "    if 'query_frame' in sample.keys():\n",
    "        sample['query_frame_origin'] = sample['query_frame'].clone()\n",
    "        sample['query_frame'] = normalization(sample['query_frame'])\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "def replicate_sample_for_hnm(gts):\n",
    "    '''\n",
    "        gts = {\n",
    "            'clip':                 in [b,t,c,h,w]\n",
    "            'clip_rigin':           in [b,t,c,h,w]\n",
    "            'clip_with_bbox':       in [b,t]\n",
    "            'before_query':         in [b,t]\n",
    "            'clip_bbox':            in [b,t,4]\n",
    "            'query':                in [b,c,h,w]\n",
    "            'query_origin':         in [b,c,h,w]\n",
    "            'clip_h':               in [b]\n",
    "            'clip_w':               in [b]\n",
    "        }\n",
    "    '''\n",
    "    clip = gts['clip']\n",
    "    clip_origin = gts['clip_origin']\n",
    "    clip_with_bbox = gts['clip_with_bbox']\n",
    "    before_query = gts['before_query']\n",
    "    clip_bbox = gts['clip_bbox']\n",
    "    query = gts['query']\n",
    "    query_origin = gts['query_origin']\n",
    "    clip_h, clip_w = gts['clip_h'], gts['clip_w']\n",
    "\n",
    "    b, t = clip.shape[:2]\n",
    "    device = clip.device\n",
    "\n",
    "    new_clip = []\n",
    "    new_clip_origin = []\n",
    "    new_clip_with_bbox = []\n",
    "    new_before_query = []\n",
    "    new_clip_bbox = []\n",
    "    new_query = []\n",
    "    new_query_origin = []\n",
    "    new_clip_h, new_clip_w = [], []\n",
    "\n",
    "    for i in range(b):\n",
    "        for j in range(b):\n",
    "            new_clip.append(clip[i])\n",
    "            new_clip_origin.append(clip_origin[i])\n",
    "            new_query.append(query[j])\n",
    "            new_query_origin.append(query_origin[j])\n",
    "            if i == j:\n",
    "                new_clip_with_bbox.append(clip_with_bbox[i])\n",
    "                new_before_query.append(before_query[i])\n",
    "                new_clip_bbox.append(clip_bbox[i])\n",
    "            else:\n",
    "                new_clip_with_bbox.append(torch.zeros(t).float().to(device))\n",
    "                new_before_query.append(torch.ones(t).bool().to(device))\n",
    "                new_clip_bbox.append(torch.tensor([[0.0, 0.0, 0.0001, 0.0001]]).repeat(t,1).float().to(device))\n",
    "            new_clip_h.append(clip_h[i])\n",
    "            new_clip_w.append(clip_w[i])\n",
    "    \n",
    "    new_clip = torch.stack(new_clip)\n",
    "    new_clip_origin = torch.stack(new_clip_origin)\n",
    "    new_clip_with_bbox = torch.stack(new_clip_with_bbox)\n",
    "    new_before_query = torch.stack(new_before_query)\n",
    "    new_clip_bbox = torch.stack(new_clip_bbox)\n",
    "    new_clip_h = torch.stack(new_clip_h)\n",
    "    new_clip_w = torch.stack(new_clip_w)\n",
    "    new_query = torch.stack(new_query)\n",
    "    new_query_origin = torch.stack(new_query_origin)\n",
    "\n",
    "    new_gts = {\n",
    "            'clip': new_clip,                       # in [b^2,t,c,h,w]\n",
    "            'clip_origin': new_clip_origin,         # in [b^2,t,c,h,w]\n",
    "            'clip_with_bbox': new_clip_with_bbox,   # in [b^2,t]\n",
    "            'before_query': new_before_query,       # in [b^2,t]\n",
    "            'clip_bbox': new_clip_bbox,             # in [b^2,t,4]\n",
    "            'query': new_query,                     # in [b^2,c,h,w]\n",
    "            'query_origin': new_query_origin,       # in [b^2,c,h,w]\n",
    "            'clip_h': new_clip_h,                   # in [b^2]\n",
    "            'clip_w': new_clip_w,                   # in [b^2]\n",
    "        }\n",
    "    return new_gts\n",
    "\n",
    "\n",
    "def normalize_bbox(bbox, h, w):\n",
    "    '''\n",
    "    bbox torch tensor in shape [4] or [...,4], under torch axis\n",
    "    '''\n",
    "    bbox_cp = bbox.clone()\n",
    "    if len(bbox.shape) > 1: # [N,4]\n",
    "        bbox_cp[..., 0] /= h\n",
    "        bbox_cp[..., 1] /= w\n",
    "        bbox_cp[..., 2] /= h\n",
    "        bbox_cp[..., 3] /= w\n",
    "        return bbox_cp\n",
    "    else:\n",
    "        return torch.tensor([bbox_cp[0]/h, bbox_cp[1]/w, bbox_cp[2]/h, bbox_cp[3]/w])\n",
    "\n",
    "\n",
    "def recover_bbox(bbox, h, w):\n",
    "    '''\n",
    "    bbox torch tensor in shape [4] or [...,4], under torch axis\n",
    "    '''\n",
    "    bbox_cp = bbox.clone()\n",
    "    if len(bbox.shape) > 1: # [N,4]\n",
    "        bbox_cp[..., 0] *= h\n",
    "        bbox_cp[..., 1] *= w\n",
    "        bbox_cp[..., 2] *= h\n",
    "        bbox_cp[..., 3] *= w\n",
    "        return bbox_cp\n",
    "    else:\n",
    "        return torch.tensor([bbox_cp[0]*h, bbox_cp[1]*w, bbox_cp[2]*h, bbox_cp[3]*w])\n",
    "    \n",
    "\n",
    "def bbox_torchTocv2(bbox):\n",
    "    '''\n",
    "    torch, idx 0/2 for height, 1/3 for width (x,y,x,y)\n",
    "    cv2: idx 0/2 for width, 1/3 for height (y,x,y,x)\n",
    "    bbox torch tensor in shape [4] or [...,4], under torch axis\n",
    "    '''\n",
    "    bbox_cp = bbox.clone()\n",
    "    if len(bbox.shape) > 1:\n",
    "        bbox_x1 = bbox_cp[...,0].unsqueeze(-1)\n",
    "        bbox_y1 = bbox_cp[...,1].unsqueeze(-1)\n",
    "        bbox_x2 = bbox_cp[...,2].unsqueeze(-1)\n",
    "        bbox_y2 = bbox_cp[...,3].unsqueeze(-1)\n",
    "        return torch.cat([bbox_y1, bbox_x1, bbox_y2, bbox_x2], dim=-1)\n",
    "    else:\n",
    "        return torch.tensor([bbox_cp[1], bbox_cp[0], bbox_cp[3], bbox_cp[2]])\n",
    "    \n",
    "\n",
    "def bbox_cv2Totorch(bbox):\n",
    "    '''\n",
    "    torch, idx 0/2 for height, 1/3 for width (x,y,x,y)\n",
    "    cv2: idx 0/2 for width, 1/3 for height (y,x,y,x)\n",
    "    bbox torch tensor in shape [4] or [...,4], under cv2 axis\n",
    "    '''\n",
    "    bbox_cp = bbox.clone()\n",
    "    if len(bbox.shape) > 1:\n",
    "        bbox_x1 = bbox_cp[...,1].unsqueeze(-1)\n",
    "        bbox_y1 = bbox_cp[...,0].unsqueeze(-1)\n",
    "        bbox_x2 = bbox_cp[...,3].unsqueeze(-1)\n",
    "        bbox_y2 = bbox_cp[...,2].unsqueeze(-1)\n",
    "        return torch.cat([bbox_x1, bbox_y1, bbox_x2, bbox_y2], dim=-1)\n",
    "    else:\n",
    "        return torch.tensor([bbox_cp[1], bbox_cp[0], bbox_cp[3], bbox_cp[2]])\n",
    "\n",
    "\n",
    "def check_bbox(bbox, h, w):\n",
    "    B, T, _ = bbox.shape\n",
    "    bbox = bbox.reshape(-1,4)\n",
    "\n",
    "    x1, y1, x2, y2 = bbox[...,0], bbox[...,1], bbox[...,2], bbox[...,3]\n",
    "    left_invalid = y2 <= 0.0\n",
    "    right_invalid = y1 >= w - 1\n",
    "    top_invalid = x2 <= 0.0\n",
    "    bottom_invalid = x1 >= h - 1\n",
    "\n",
    "    x_invalid = torch.logical_or(top_invalid, bottom_invalid)\n",
    "    y_invalid = torch.logical_or(left_invalid, right_invalid)\n",
    "    invalid = torch.logical_or(x_invalid, y_invalid)\n",
    "    valid = ~invalid\n",
    "\n",
    "    x1_clip = x1.clip(min=0.0, max=h).unsqueeze(-1)\n",
    "    x2_clip = x2.clip(min=0.0, max=h).unsqueeze(-1)\n",
    "    y1_clip = y1.clip(min=0.0, max=w).unsqueeze(-1)\n",
    "    y2_clip = y2.clip(min=0.0, max=w).unsqueeze(-1)\n",
    "    bbox_clip = torch.cat([x1_clip, y1_clip, x2_clip, y2_clip], dim=-1)\n",
    "\n",
    "    return bbox_clip.reshape(B,T,4), valid.reshape(B,T)\n",
    "\n",
    "\n",
    "def check_bbox_permute(bbox_p):\n",
    "    '''\n",
    "    bbox_p: [N,4], (x1,y1,x2,y2)\n",
    "    '''\n",
    "    x1p = torch.minimum(bbox_p[:, 0], bbox_p[:, 2]).reshape(-1,1)\n",
    "    x2p = torch.maximum(bbox_p[:, 0], bbox_p[:, 2]).reshape(-1,1)\n",
    "    y1p = torch.minimum(bbox_p[:, 1], bbox_p[:, 3]).reshape(-1,1)\n",
    "    y2p = torch.maximum(bbox_p[:, 1], bbox_p[:, 3]).reshape(-1,1)\n",
    "    bbox_p = torch.cat([x1p, y1p, x2p, y2p], axis=1)\n",
    "    return bbox_p\n",
    "\n",
    "\n",
    "def bbox_xyxyTopoints(bbox):\n",
    "    '''\n",
    "    bbox: torch.Tensor, in shape [..., 4]\n",
    "    return: bbox in shape [...,4,2] with 4 points location\n",
    "    p1---p2\n",
    "    |     |\n",
    "    p4---p3\n",
    "    '''\n",
    "    bbox_x1 = bbox[...,0].unsqueeze(-1)     # [...,1]\n",
    "    bbox_y1 = bbox[...,1].unsqueeze(-1)\n",
    "    bbox_x2 = bbox[...,2].unsqueeze(-1)\n",
    "    bbox_y2 = bbox[...,3].unsqueeze(-1)\n",
    "\n",
    "    pt1 = torch.cat([bbox_x1, bbox_y1], dim=-1).unsqueeze(-2)     # [...,1,2]\n",
    "    pt2 = torch.cat([bbox_x2, bbox_y1], dim=-1).unsqueeze(-2)     # [...,1,2]\n",
    "    pt3 = torch.cat([bbox_x2, bbox_y2], dim=-1).unsqueeze(-2)     # [...,1,2]\n",
    "    pt4 = torch.cat([bbox_x1, bbox_y2], dim=-1).unsqueeze(-2)     # [...,1,2]\n",
    "\n",
    "    pts = torch.cat([pt1, pt2, pt3, pt4], dim=-2)                 # [...,4,2]\n",
    "    return pts\n",
    "\n",
    "\n",
    "def bbox_pointsToxyxy(pts):\n",
    "    '''\n",
    "    pts: torch.Tensor, in shape [...,4,2]\n",
    "    return: bbox in shape [...,4] for x1y1x2y2\n",
    "    '''\n",
    "    shape_in = list(pts.shape[:-2])\n",
    "    pts = pts.reshape(-1,4,2)\n",
    "\n",
    "    pt1 = pts[:,0,:]           # [N,2]\n",
    "    pt3 = pts[:,3,:]\n",
    "\n",
    "    x1 = pt1[:, 0].unsqueeze(-1)  # [N,1]\n",
    "    y1 = pt1[:, 1].unsqueeze(-1)  \n",
    "    x2 = pt3[:, 0].unsqueeze(-1)  \n",
    "    y2 = pt3[:, 1].unsqueeze(-1)\n",
    "\n",
    "    bbox = torch.cat([x1,y1,x2,y2], dim=-1)     # [N,4]\n",
    "    bbox = bbox.reshape(shape_in + [4])\n",
    "    return bbox\n",
    "\n",
    "\n",
    "def create_square_bbox(bbox, img_h, img_w):\n",
    "    '''\n",
    "    bbox in [4], in torch coordinate\n",
    "    '''\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    center_x = (x1 + x2) / 2\n",
    "    center_y = (y1 + y2) / 2\n",
    "    h = center_x - x1\n",
    "    w = center_y - y1\n",
    "    r = max(h, w)\n",
    "\n",
    "    new_x1 = max(center_x - r, 0)\n",
    "    new_x2 = min(center_x + r, img_h-1)\n",
    "    new_y1 = max(center_y - r, 0)\n",
    "    new_y2 = min(center_y + r, img_w-1)\n",
    "\n",
    "    new_bbox = torch.tensor([new_x1, new_y1, new_x2, new_y2])\n",
    "    return new_bbox\n",
    "\n",
    "\n",
    "def bbox_xyhwToxyxy(bbox_xyhw):\n",
    "    '''\n",
    "    bbox_xyhw in shape [..., 4]\n",
    "    height and width of bbox is the full height and width\n",
    "    '''\n",
    "    bbox_center = bbox_xyhw[..., :2]\n",
    "    bbox_hw = bbox_xyhw[..., 2:]\n",
    "    bbox_hw_half = 0.5 * bbox_hw\n",
    "\n",
    "    bbox_xyxy = torch.cat([bbox_center - bbox_hw_half, bbox_center + bbox_hw_half], dim=-1)\n",
    "    return bbox_xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T11:05:56.922833Z",
     "iopub.status.busy": "2025-11-05T11:05:56.922504Z",
     "iopub.status.idle": "2025-11-05T11:05:56.941304Z",
     "shell.execute_reply": "2025-11-05T11:05:56.940622Z",
     "shell.execute_reply.started": "2025-11-05T11:05:56.922817Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class PositionalEncoding1D(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        \"\"\"\n",
    "        :param channels: The last dimension of the tensor you want to apply pos emb to.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding1D, self).__init__()\n",
    "        self.org_channels = channels\n",
    "        channels = int(np.ceil(channels / 2) * 2)\n",
    "        self.channels = channels\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, channels, 2).float() / channels))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.cached_penc = None\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        \"\"\"\n",
    "        :param tensor: A 3d tensor of size (batch_size, x, ch)\n",
    "        :return: Positional Encoding Matrix of size (batch_size, x, ch)\n",
    "        \"\"\"\n",
    "        if len(tensor.shape) != 3:\n",
    "            raise RuntimeError(\"The input tensor has to be 3d!\")\n",
    "\n",
    "        if self.cached_penc is not None and self.cached_penc.shape == tensor.shape:\n",
    "            return self.cached_penc\n",
    "\n",
    "        self.cached_penc = None\n",
    "        batch_size, x, orig_ch = tensor.shape\n",
    "        pos_x = torch.arange(x, device=tensor.device).type(self.inv_freq.type())\n",
    "        sin_inp_x = torch.einsum(\"i,j->ij\", pos_x, self.inv_freq)\n",
    "        emb_x = get_emb(sin_inp_x)\n",
    "        emb = torch.zeros((x, self.channels), device=tensor.device).type(tensor.type())\n",
    "        emb[:, : self.channels] = emb_x\n",
    "\n",
    "        self.cached_penc = emb[None, :, :orig_ch].repeat(batch_size, 1, 1)\n",
    "        return self.cached_penc\n",
    "    \n",
    "\n",
    "def positionalencoding1d(d_model, length):\n",
    "    \"\"\"\n",
    "    positional encoding for 1-d sequence\n",
    "    :param d_model: dimension of the model (C)\n",
    "    :param length: length of positions (N)\n",
    "    :return: length*d_model position matrix, shape [N, C]\n",
    "    \"\"\"\n",
    "    if d_model % 2 != 0:\n",
    "        raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
    "                         \"odd dim (got dim={:d})\".format(d_model))\n",
    "    pe = torch.zeros(length, d_model)\n",
    "    position = torch.arange(0, length).unsqueeze(1)\n",
    "    div_term = torch.exp((torch.arange(0, d_model, 2, dtype=torch.float) *\n",
    "                         -(math.log(10000.0) / d_model)))       # [N,C//2]\n",
    "    pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "    return pe\n",
    "\n",
    "\n",
    "def positionalencoding2d(d_model, height, width, type='sinusoidal'):\n",
    "    \"\"\"\n",
    "    :param d_model: dimension of the model\n",
    "    :param height: height of the positions\n",
    "    :param width: width of the positions\n",
    "    :return: d_model*height*width position matrix, shape [H*W, C]\n",
    "    \"\"\"\n",
    "    if type == 'sinusoidal':\n",
    "        pe = torch.zeros(d_model, height, width)\n",
    "        # Each dimension use half of d_model\n",
    "        d_model_origin = d_model\n",
    "        d_model = int(d_model / 2)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) *\n",
    "                            -(math.log(10000.0) / d_model))\n",
    "        pos_w = torch.arange(0., width).unsqueeze(1)\n",
    "        pos_h = torch.arange(0., height).unsqueeze(1)\n",
    "        pe[0:d_model:2, :, :] = torch.sin(pos_h * div_term).transpose(0, 1).unsqueeze(1).repeat(1, width, 1)\n",
    "        pe[1:d_model:2, :, :] = torch.cos(pos_h * div_term).transpose(0, 1).unsqueeze(1).repeat(1, width, 1)\n",
    "        pe[d_model::2, :, :] = torch.sin(pos_w * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, height)\n",
    "        pe[d_model + 1::2, :, :] = torch.cos(pos_w * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, height)\n",
    "        pe = rearrange(pe, 'c h w -> (h w) c')[:,:d_model_origin]\n",
    "    elif type == 'zero':\n",
    "        pe = torch.zeros(height * width, d_model)\n",
    "    return pe\n",
    "\n",
    "\n",
    "def positionalencoding3d(d_model, height, width, depth, type='sinusoidal'):\n",
    "    \"\"\"\n",
    "    :param d_model: dimension of the model\n",
    "    :param height: height of the positions\n",
    "    :param width: width of the positions\n",
    "    :param depth: depth of the positions\n",
    "    :return: d_model*height*width position matrix, shape [H*W, C]\n",
    "    \"\"\"\n",
    "    if type == 'sinusoidal':\n",
    "        d_model_interv = int(np.ceil(d_model / 6) * 2)\n",
    "        if d_model_interv % 2:\n",
    "            d_model_interv += 1\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, d_model_interv, 2).float() / d_model_interv))\n",
    "        pos_x = torch.arange(height).type(inv_freq.type())\n",
    "        pos_y = torch.arange(width).type(inv_freq.type())\n",
    "        pos_z = torch.arange(depth).type(inv_freq.type())\n",
    "        sin_inp_x = torch.einsum(\"i,j->ij\", pos_x, inv_freq)\n",
    "        sin_inp_y = torch.einsum(\"i,j->ij\", pos_y, inv_freq)\n",
    "        sin_inp_z = torch.einsum(\"i,j->ij\", pos_z, inv_freq)\n",
    "        emb_x = get_emb(sin_inp_x).unsqueeze(1).unsqueeze(1)\n",
    "        emb_y = get_emb(sin_inp_y).unsqueeze(1)\n",
    "        emb_z = get_emb(sin_inp_z)\n",
    "        emb = torch.zeros(height, width, depth, d_model_interv * 3)\n",
    "        emb[:, :, :, : d_model_interv] = emb_x\n",
    "        emb[:, :, :, d_model_interv : 2 * d_model_interv] = emb_y\n",
    "        emb[:, :, :, 2 * d_model_interv :] = emb_z\n",
    "        emb = rearrange(emb, 'h w d c -> (h w d) c')[:,:d_model]\n",
    "    elif type == 'zero':\n",
    "        emb = torch.zeros(height * width * depth, d_model)\n",
    "    return emb\n",
    "\n",
    "\n",
    "def get_emb(sin_inp):\n",
    "    \"\"\"\n",
    "    Gets a base embedding for one dimension with sin and cos intertwined\n",
    "    \"\"\"\n",
    "    emb = torch.stack((sin_inp.sin(), sin_inp.cos()), dim=-1)\n",
    "    return torch.flatten(emb, -2, -1)\n",
    "\n",
    "\n",
    "def BasicBlock_Conv2D(in_dim, out_dim):\n",
    "    module = nn.Sequential(\n",
    "                    nn.Conv2d(in_dim, out_dim, 3, padding=1),\n",
    "                    nn.BatchNorm2d(out_dim),\n",
    "                    nn.LeakyReLU(inplace=True)\n",
    "                    )\n",
    "    return module\n",
    "\n",
    "def BasicBlock_MLP(dims):\n",
    "    dims_ = dims[:-1]\n",
    "    dims1, dims2 = dims_[:-1], dims_[1:]\n",
    "    mlp = []\n",
    "    for (dim1, dim2) in zip(dims1, dims2):\n",
    "        mlp.append(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim1, dim2),\n",
    "                nn.BatchNorm1d(dim1),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "        ))\n",
    "    mlp.append(\n",
    "        nn.Sequential(\n",
    "            nn.Linear(dims[-2], dims[-1]),\n",
    "        ))\n",
    "    mlp = nn.Sequential(*mlp)\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T11:05:56.943779Z",
     "iopub.status.busy": "2025-11-05T11:05:56.943470Z",
     "iopub.status.idle": "2025-11-05T11:05:56.974078Z",
     "shell.execute_reply": "2025-11-05T11:05:56.973415Z",
     "shell.execute_reply.started": "2025-11-05T11:05:56.943752Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "def generate_anchor_boxes_on_regions(image_size, \n",
    "                                     num_regions, \n",
    "                                     base_sizes=torch.tensor([[16, 16], [32, 32], [64, 64], [128, 128]], dtype=torch.float32),\n",
    "                                     aspect_ratios=torch.tensor([0.5, 1, 2], dtype=torch.float32),\n",
    "                                     dtype=torch.float32, \n",
    "                                     device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate a set of anchor boxes with different sizes and aspect ratios for each region of a split image.\n",
    "\n",
    "    Arguments:\n",
    "    image_size -- tuple of two integers, the height and width of the original image\n",
    "    num_regions -- tuple of two integers, the number of regions in the height and width directions\n",
    "    aspect_ratios -- torch.Tensor of shape [M], containing M aspect ratios for each base size\n",
    "    dtype -- the data type of the output tensor\n",
    "    device -- the device of the output tensor\n",
    "\n",
    "    Returns:\n",
    "    anchor_boxes -- torch.Tensor of shape [R^2*N*M,4], containing R^2*N*M anchor boxes represented as (center_h, center_w, box_h, box_w)\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the base sizes for each region\n",
    "    region_size = (image_size[0] / num_regions[0], image_size[1] / num_regions[1])\n",
    "\n",
    "    # Calculate the anchor boxes for each region\n",
    "    anchor_boxes = torch.empty((0, 4), dtype=dtype, device=device)\n",
    "    for i in range(num_regions[0]):\n",
    "        for j in range(num_regions[1]):\n",
    "            center_h = (i + 0.5) * region_size[0]\n",
    "            center_w = (j + 0.5) * region_size[1]\n",
    "            base_boxes = generate_anchor_boxes(base_sizes, aspect_ratios, dtype=dtype, device=device)\n",
    "            base_boxes[:, 0] += center_h\n",
    "            base_boxes[:, 1] += center_w\n",
    "            anchor_boxes = torch.cat([anchor_boxes, base_boxes], dim=0)\n",
    "\n",
    "    return anchor_boxes\n",
    "\n",
    "\n",
    "def generate_anchor_boxes(base_sizes, aspect_ratios, dtype=torch.float32, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate a set of anchor boxes with different sizes and aspect ratios.\n",
    "\n",
    "    Arguments:\n",
    "    base_sizes -- torch.Tensor of shape [N,2], containing N base sizes for the anchor boxes\n",
    "    aspect_ratios -- torch.Tensor of shape [M], containing M aspect ratios for each base size\n",
    "    dtype -- the data type of the output tensor\n",
    "    device -- the device of the output tensor\n",
    "\n",
    "    Returns:\n",
    "    anchor_boxes -- torch.Tensor of shape [N*M,4], containing N*M anchor boxes represented as (center_h, center_w, box_h, box_w)\n",
    "    \"\"\"\n",
    "\n",
    "    num_base_sizes = base_sizes.shape[0]\n",
    "    num_aspect_ratios = aspect_ratios.shape[0]\n",
    "\n",
    "    # Generate base anchor boxes\n",
    "    base_boxes = torch.zeros((num_base_sizes * num_aspect_ratios, 4), dtype=dtype, device=device)\n",
    "    for i in range(num_base_sizes):\n",
    "        for j in range(num_aspect_ratios):\n",
    "            w = torch.sqrt(base_sizes[i, 0] * base_sizes[i, 1] / aspect_ratios[j])\n",
    "            h = aspect_ratios[j] * w\n",
    "            idx = i * num_aspect_ratios + j\n",
    "            base_boxes[idx] = torch.tensor([0, 0, h, w], dtype=dtype, device=device)\n",
    "\n",
    "    return base_boxes\n",
    "\n",
    "\n",
    "# def assign_labels(proposals, gt_boxes, iou_threshold=0.5):\n",
    "#     \"\"\"\n",
    "#     Assign labels to a set of bounding box proposals based on their IoU with ground truth boxes.\n",
    "\n",
    "#     Arguments:\n",
    "#     proposals -- torch.Tensor of shape [B,T,N,4], representing the bounding box proposals for each frame in each clip\n",
    "#     gt_boxes -- torch.Tensor of shape [B,T,4], representing the ground truth boxes for each frame in each clip\n",
    "#     iou_threshold -- float, the IoU threshold for a proposal to be considered a positive match with a ground truth box\n",
    "\n",
    "#     Returns:\n",
    "#     labels -- torch.Tensor of shape [B,T,N], containing the assigned labels for each proposal (0 for background, 1 for object)\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Initialize the labels tensor with background labels\n",
    "#     labels = torch.zeros_like(proposals[:, :, :, 0], dtype=torch.long, device=proposals.device)\n",
    "\n",
    "#     # Loop over the batches and frames\n",
    "#     for b in range(proposals.shape[0]):\n",
    "#         for t in range(proposals.shape[1]):\n",
    "#             # Calculate the IoU between each proposal and the ground truth box\n",
    "#             iou = calculate_iou(proposals[b, t], gt_boxes[b, t])    # [N]\n",
    "\n",
    "#             # Assign labels to the proposals based on their IoU with the ground truth box\n",
    "#             labels[b, t] = iou > iou_threshold\n",
    "\n",
    "#     return labels\n",
    "\n",
    "\n",
    "def assign_labels(anchors, gt_boxes, iou_threshold=0.5, topk=5):\n",
    "    \"\"\"\n",
    "    Assign labels to a set of bounding box proposals based on their IoU with ground truth boxes.\n",
    "\n",
    "    Arguments:\n",
    "    anchors -- torch.Tensor of shape [B,T,N,4], representing the bounding box proposals for each frame in each clip\n",
    "    gt_boxes -- torch.Tensor of shape [B,T,4], representing the ground truth boxes for each frame in each clip\n",
    "    iou_threshold -- float, the IoU threshold for a proposal to be considered a positive match with a ground truth box\n",
    "\n",
    "    Returns:\n",
    "    labels -- torch.Tensor of shape [B,T,N], containing the assigned labels for each proposal (0 for background, 1 for object)\n",
    "    \"\"\"\n",
    "    anchors = anchors.detach()\n",
    "    gt_boxes = gt_boxes.detach()\n",
    "\n",
    "    b,t = gt_boxes.shape[:2]    #[B,T,N,4]\n",
    "\n",
    "    # Calculate the IoU between each proposal and the ground truth box\n",
    "    iou = calculate_iou(anchors.view(-1, anchors.shape[-2], anchors.shape[-1]),   # [B*T,N,4]\n",
    "                        gt_boxes.view(-1, gt_boxes.shape[-1]))                    # [B*T,4] -> [B*T,N]\n",
    "    iou = iou.view(anchors.shape[:-1])    # [B,T,N]\n",
    "\n",
    "    # Assign labels to the proposals based on their IoU with the ground truth box\n",
    "    labels = iou > iou_threshold\n",
    "\n",
    "    if not labels.any():\n",
    "        labels = process_labels(labels, iou, topk)\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def calculate_iou(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    Calculate the IoU between two sets of bounding boxes.\n",
    "\n",
    "    Arguments:\n",
    "    boxes1 -- torch.Tensor of shape [...,N,4], containing N bounding boxes represented as [x1, y1, x2, y2]\n",
    "    boxes2 -- torch.Tensor of shape [...,4], containing a single ground truth box represented as [x1, y1, x2, y2]\n",
    "\n",
    "    Returns:\n",
    "    iou -- torch.Tensor of shape [...,N], containing the IoU between each box and the ground truth box\n",
    "    \"\"\"\n",
    "\n",
    "    # Add a new dimension to boxes2 for broadcasting\n",
    "    boxes2 = boxes2.unsqueeze(-2)    # shape: [...,1,4]\n",
    "\n",
    "    # Compute the coordinates of the top-left and bottom-right corners of the boxes\n",
    "    boxes1_tl = boxes1[..., :2]\n",
    "    boxes1_br = boxes1[..., 2:]\n",
    "    boxes2_tl = boxes2[..., :2]\n",
    "    boxes2_br = boxes2[..., 2:]\n",
    "\n",
    "    # Compute the coordinates of the intersection rectangle\n",
    "    tl = torch.max(boxes1_tl, boxes2_tl)\n",
    "    br = torch.min(boxes1_br, boxes2_br)\n",
    "\n",
    "    # Compute the width and height of the intersection rectangle\n",
    "    wh = br - tl\n",
    "    wh[wh < 0] = 0\n",
    "\n",
    "    # Compute the area of the intersection and union rectangles\n",
    "    intersection_area = wh[..., 0] * wh[..., 1]\n",
    "    area1 = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n",
    "    area2 = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n",
    "    union_area = area1 + area2 - intersection_area\n",
    "\n",
    "    # Compute the IoU between each box and the ground truth box\n",
    "    iou = intersection_area / union_area\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def process_labels(labels, iou, topk=10):\n",
    "    '''\n",
    "    labels: in shape [B,T,N], bool\n",
    "    iou: in shape [B,T,N]\n",
    "    '''\n",
    "    B,T,N = labels.shape\n",
    "\n",
    "    labels = rearrange(labels, 'b t n -> (b t n)')\n",
    "    iou = rearrange(iou, 'b t n -> (b t n)')\n",
    "\n",
    "    if not labels.any():\n",
    "        # no pos assigned, choose topk anchors with largest iou as positives\n",
    "        _, topk_indices = torch.topk(iou, k=topk)\n",
    "        labels[topk_indices] = True\n",
    "    \n",
    "    labels = rearrange(labels, '(b t n) -> b t n', b=B, t=T, n=N)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T11:05:56.975077Z",
     "iopub.status.busy": "2025-11-05T11:05:56.974809Z",
     "iopub.status.idle": "2025-11-05T11:05:56.997592Z",
     "shell.execute_reply": "2025-11-05T11:05:56.996902Z",
     "shell.execute_reply.started": "2025-11-05T11:05:56.975055Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads=1, mlp_ratio=4., act_layer=nn.GELU, norm_layer=nn.LayerNorm, return_attn=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels = dim\n",
    "\n",
    "        self.encode_query = nn.Conv1d(in_channels=dim, out_channels=dim, kernel_size=1, stride=1, padding=0)\n",
    "        self.encode_key = nn.Conv1d(in_channels=dim, out_channels=dim, kernel_size=1, stride=1, padding=0)\n",
    "        self.encode_value = nn.Conv1d(in_channels=dim, out_channels=dim, kernel_size=1, stride=1, padding=0)\n",
    "        self.norm = norm_layer(dim)\n",
    "\n",
    "        self.attn = Attention(dim, num_heads=num_heads)\n",
    "        \n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer)\n",
    "\n",
    "    def with_pos_embed(self, tensor, pos):\n",
    "        return tensor if pos is None else tensor + pos.to(tensor)\n",
    "    \n",
    "    def get_attn(self, query, key, query_embed=None, key_embed=None):\n",
    "        b, c, n = query.shape\n",
    "\n",
    "        q = self.with_pos_embed(query, query_embed)\n",
    "        k = self.with_pos_embed(key, key_embed)\n",
    "\n",
    "        q = self.norm(q.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        k = self.norm(k.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "\n",
    "        q = self.encode_query(q).view(b, self.channels, -1)\n",
    "        q = q.permute(0, 2, 1)\n",
    "        k = self.encode_key(k).view(b, self.channels, -1)\n",
    "        k = k.permute(0, 2, 1)\n",
    "        return self.attn.get_attn(query=q, key=k)   # [b,n,n]\n",
    "    \n",
    "    def forward(self, query, key, query_embed=None, key_embed=None):\n",
    "        b, c, n = query.shape\n",
    "\n",
    "        q = self.with_pos_embed(query, query_embed)\n",
    "        k = self.with_pos_embed(key, key_embed)\n",
    "\n",
    "        q = self.norm(q.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        k = self.norm(k.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "\n",
    "        v = self.encode_value(key).view(b, self.channels, -1)\n",
    "        v = v.permute(0, 2, 1)\n",
    "\n",
    "        q = self.encode_query(q).view(b, self.channels, -1)\n",
    "        q = q.permute(0, 2, 1)\n",
    "\n",
    "        k = self.encode_key(k).view(b, self.channels, -1)\n",
    "        k = k.permute(0, 2, 1)\n",
    "\n",
    "        query = query.view(b, self.channels, -1).permute(0, 2, 1)\n",
    "        query = query + self.attn(query=q, key=k, value=v)\n",
    "\n",
    "        query = query + self.mlp(self.norm2(query))\n",
    "        query = query.permute(0, 2, 1).contiguous().view(b, self.channels, -1)\n",
    "\n",
    "        return query\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "    def get_attn(self, query, key):\n",
    "        B, N, C = query.shape\n",
    "        attn = torch.matmul(query, key.transpose(-2, -1)) #* self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        #__import__('pdb').set_trace()\n",
    "        return attn\n",
    "    \n",
    "    def forward(self, query, key, value):\n",
    "        B, N, C = query.shape\n",
    "        query = query.reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        key = key.reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        value = value.reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        attn = torch.matmul(query, key.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = torch.matmul(attn, value).transpose(1, 2).reshape(B, N, C)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        # nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.normal_(self.fc1.weight, mean=0.0, std=1e-3)\n",
    "        nn.init.normal_(self.fc2.weight, mean=0.0, std=1e-3)\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-4)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T11:05:56.998620Z",
     "iopub.status.busy": "2025-11-05T11:05:56.998354Z",
     "iopub.status.idle": "2025-11-05T11:06:01.803201Z",
     "shell.execute_reply": "2025-11-05T11:06:01.802284Z",
     "shell.execute_reply.started": "2025-11-05T11:05:56.998592Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "import timm.models.vision_transformer\n",
    "\n",
    "\n",
    "class VisionTransformer(timm.models.vision_transformer.VisionTransformer):\n",
    "    \"\"\" Vision Transformer with support for global average pooling\n",
    "    \"\"\"\n",
    "    def __init__(self, global_pool=False, **kwargs):\n",
    "        super(VisionTransformer, self).__init__(**kwargs)\n",
    "\n",
    "        self.global_pool = global_pool\n",
    "        if self.global_pool:\n",
    "            norm_layer = kwargs['norm_layer']\n",
    "            embed_dim = kwargs['embed_dim']\n",
    "            self.fc_norm = norm_layer(embed_dim)\n",
    "\n",
    "            del self.norm  # remove the original norm\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        \n",
    "        # [b,h*w+1,c]\n",
    "        return x\n",
    "\n",
    "\n",
    "def vit_base_patch16(**kwargs):\n",
    "    model = VisionTransformer(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-05T11:06:01.804443Z",
     "iopub.status.busy": "2025-11-05T11:06:01.804118Z",
     "iopub.status.idle": "2025-11-05T11:06:01.844019Z",
     "shell.execute_reply": "2025-11-05T11:06:01.843214Z",
     "shell.execute_reply.started": "2025-11-05T11:06:01.804425Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import math\n",
    "import torchvision\n",
    "\n",
    "base_sizes=torch.tensor([[16, 16], [32, 32], [64, 64], [128, 128]], dtype=torch.float32)    # 4 types of size\n",
    "aspect_ratios=torch.tensor([0.5, 1, 2], dtype=torch.float32)                                # 3 types of aspect ratio\n",
    "n_base_sizes = base_sizes.shape[0]\n",
    "n_aspect_ratios = aspect_ratios.shape[0]\n",
    "\n",
    "\n",
    "def build_backbone(config):\n",
    "    name, type = config.model.backbone_name, config.model.backbone_type\n",
    "    if name == 'dino':\n",
    "        assert type in ['vitb8', 'vitb16', 'vits8', 'vits16']\n",
    "        backbone = torch.hub.load('facebookresearch/dino:main', 'dino_{}'.format(type))\n",
    "        down_rate = int(type.replace('vitb', '').replace('vits', ''))\n",
    "        backbone_dim = 768\n",
    "        if type == 'vitb16' and config.model.bakcbone_use_mae_weight:\n",
    "            mae_weight = torch.load('/vision/hwjiang/episodic-memory/VQ2D/checkpoint/mae_pretrain_vit_base.pth')['model']\n",
    "            backbone.load_state_dict(mae_weight)\n",
    "    elif name == 'dinov2':\n",
    "        assert type in ['vits14', 'vitb14', 'vitl14', 'vitg14']\n",
    "        backbone = torch.hub.load('facebookresearch/dinov2', 'dinov2_{}'.format(type))\n",
    "        down_rate = 14\n",
    "        if type == 'vitb14':\n",
    "            backbone_dim = 768\n",
    "        elif type == 'vits14':\n",
    "            backbone_dim = 384\n",
    "    elif name == 'mae':\n",
    "        backbone = vit_base_patch16()\n",
    "        cpt = torch.load('/vision/hwjiang/download/model_weight/mae_pretrain_vit_base.pth')['model']\n",
    "        backbone.load_state_dict(cpt, strict=False)\n",
    "        down_rate = 16\n",
    "        backbone_dim = 768\n",
    "    return backbone, down_rate, backbone_dim\n",
    "\n",
    "\n",
    "class ClipMatcher(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.backbone, self.down_rate, self.backbone_dim = build_backbone(config)\n",
    "        self.backbone_name = config.model.backbone_name\n",
    "\n",
    "        self.query_size = config.dataset.query_size\n",
    "        self.clip_size_fine = config.dataset.clip_size_fine\n",
    "        self.clip_size_coarse = config.dataset.clip_size_coarse\n",
    "\n",
    "        self.query_feat_size = self.query_size // self.down_rate\n",
    "        self.clip_feat_size_fine = self.clip_size_fine // self.down_rate\n",
    "        self.clip_feat_size_coarse = self.clip_size_coarse // self.down_rate\n",
    "\n",
    "        self.type_transformer = config.model.type_transformer\n",
    "        assert self.type_transformer in ['local', 'global']\n",
    "        self.window_transformer = config.model.window_transformer\n",
    "        self.resolution_transformer = config.model.resolution_transformer\n",
    "        self.resolution_anchor_feat = config.model.resolution_anchor_feat\n",
    "\n",
    "        self.anchors_xyhw = generate_anchor_boxes_on_regions(image_size=[self.clip_size_coarse, self.clip_size_coarse],\n",
    "                                                        num_regions=[self.resolution_anchor_feat, self.resolution_anchor_feat])\n",
    "        self.anchors_xyhw = self.anchors_xyhw / self.clip_size_coarse   # [R^2*N*M,4], value range [0,1], represented by [c_x,c_y,h,w] in torch axis\n",
    "        self.anchors_xyxy = bbox_xyhwToxyxy(self.anchors_xyhw)\n",
    "\n",
    "        # query down heads\n",
    "        self.query_down_heads = []\n",
    "        for _ in range(int(math.log2(self.query_feat_size))):\n",
    "            self.query_down_heads.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(self.backbone_dim, self.backbone_dim, 3, stride=2, padding=1),\n",
    "                    nn.BatchNorm2d(self.backbone_dim),\n",
    "                    nn.LeakyReLU(inplace=True),\n",
    "                )\n",
    "            )\n",
    "        self.query_down_heads = nn.ModuleList(self.query_down_heads)\n",
    "\n",
    "        # feature reduce layer\n",
    "        self.reduce = nn.Sequential(\n",
    "            nn.Conv2d(self.backbone_dim, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # clip-query correspondence\n",
    "        self.CQ_corr_transformer = []\n",
    "        for _ in range(1):\n",
    "            self.CQ_corr_transformer.append(\n",
    "                torch.nn.TransformerDecoderLayer(\n",
    "                    d_model=256,\n",
    "                    nhead=4,\n",
    "                    dim_feedforward=1024,\n",
    "                    dropout=0.0,\n",
    "                    activation='gelu',\n",
    "                    batch_first=True\n",
    "                )\n",
    "            )\n",
    "        self.CQ_corr_transformer = nn.ModuleList(self.CQ_corr_transformer)\n",
    "\n",
    "        # feature downsample layers\n",
    "        self.num_head_layers, self.down_heads = int(math.log2(self.clip_feat_size_coarse)), []\n",
    "        for i in range(self.num_head_layers-1):\n",
    "            self.in_channel = 256 if i != 0 else self.backbone_dim\n",
    "            self.down_heads.append(\n",
    "                nn.Sequential(\n",
    "                nn.Conv2d(256, 256, 3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "            ))\n",
    "        self.down_heads = nn.ModuleList(self.down_heads)\n",
    "\n",
    "        # spatial-temporal PE\n",
    "        self.pe_3d = positionalencoding3d(d_model=256, \n",
    "                                          height=self.resolution_transformer, \n",
    "                                          width=self.resolution_transformer, \n",
    "                                          depth=config.dataset.clip_num_frames,\n",
    "                                          type=config.model.pe_transformer).unsqueeze(0)\n",
    "        self.pe_3d = nn.parameter.Parameter(self.pe_3d)\n",
    "\n",
    "        # spatial-temporal transformer layer\n",
    "        self.feat_corr_transformer = []\n",
    "        self.num_transformer = config.model.num_transformer\n",
    "        for _ in range(self.num_transformer):\n",
    "            self.feat_corr_transformer.append(\n",
    "                    torch.nn.TransformerEncoderLayer(\n",
    "                        d_model=256, \n",
    "                        nhead=8,\n",
    "                        dim_feedforward=2048,\n",
    "                        dropout=0.0,\n",
    "                        activation='gelu',\n",
    "                        batch_first=True\n",
    "                ))\n",
    "        self.feat_corr_transformer = nn.ModuleList(self.feat_corr_transformer)\n",
    "        self.temporal_mask = None\n",
    "\n",
    "        # output head\n",
    "        self.head = Head(in_dim=256, in_res=self.resolution_transformer, out_res=self.resolution_anchor_feat)\n",
    "\n",
    "    def init_weights_linear(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            #nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=1e-6)\n",
    "            nn.init.normal_(m.bias, mean=0.0, std=1e-6)\n",
    "\n",
    "    def extract_feature(self, x, return_h_w=False):\n",
    "        if self.backbone_name == 'dino':\n",
    "            b, _, h_origin, w_origin = x.shape\n",
    "            out = self.backbone.get_intermediate_layers(x, n=1)[0]\n",
    "            out = out[:, 1:, :]  # we discard the [CLS] token   # [b, h*w, c]\n",
    "            h, w = int(h_origin / self.backbone.patch_embed.patch_size), int(w_origin / self.backbone.patch_embed.patch_size)\n",
    "            dim = out.shape[-1]\n",
    "            out = out.reshape(b, h, w, dim).permute(0,3,1,2)\n",
    "            if return_h_w:\n",
    "                return out, h, w\n",
    "            return out\n",
    "        elif self.backbone_name == 'dinov2':\n",
    "            b, _, h_origin, w_origin = x.shape\n",
    "            out = self.backbone.get_intermediate_layers(x, n=1)[0]\n",
    "            h, w = int(h_origin / self.backbone.patch_embed.patch_size[0]), int(w_origin / self.backbone.patch_embed.patch_size[1])\n",
    "            dim = out.shape[-1]\n",
    "            out = out.reshape(b, h, w, dim).permute(0,3,1,2)\n",
    "            if return_h_w:\n",
    "                return out, h, w\n",
    "            return out\n",
    "        elif self.backbone_name == 'mae':\n",
    "            b, _, h_origin, w_origin = x.shape\n",
    "            out = self.backbone.forward_features(x) # [b,1+h*w,c]\n",
    "            h, w = int(h_origin / self.backbone.patch_embed.patch_size[0]), int(w_origin / self.backbone.patch_embed.patch_size[1])\n",
    "            dim = out.shape[-1]\n",
    "            out = out[:,1:].reshape(b, h, w, dim).permute(0,3,1,2)  # [b,c,h,w]\n",
    "            out = F.interpolate(out, size=(16,16), mode='bilinear')\n",
    "            if return_h_w:\n",
    "                return out, h, w\n",
    "            return out\n",
    "        \n",
    "        \n",
    "    def replicate_for_hnm(self, query_feat, clip_feat):\n",
    "        '''\n",
    "        query_feat in shape [b,c,h,w]\n",
    "        clip_feat in shape [b*t,c,h,w]\n",
    "        '''\n",
    "        b = query_feat.shape[0]\n",
    "        bt = clip_feat.shape[0]\n",
    "        t = bt // b\n",
    "        \n",
    "        clip_feat = rearrange(clip_feat, '(b t) c h w -> b t c h w', b=b, t=t)\n",
    "\n",
    "        new_clip_feat, new_query_feat = [], []\n",
    "        for i in range(b):\n",
    "            for j in range(b):\n",
    "                new_clip_feat.append(clip_feat[i])\n",
    "                new_query_feat.append(query_feat[j])\n",
    "\n",
    "        new_clip_feat = torch.stack(new_clip_feat)      # [b^2,t,c,h,w]\n",
    "        new_query_feat = torch.stack(new_query_feat)    # [b^2,c,h,w]\n",
    "\n",
    "        new_clip_feat = rearrange(new_clip_feat, 'b t c h w -> (b t) c h w')\n",
    "        return new_clip_feat, new_query_feat\n",
    "\n",
    "\n",
    "    def forward(self, clip, query, query_frame_bbox=None, training=False, fix_backbone=True):\n",
    "        '''\n",
    "        clip: in shape [b,t,c,h,w]\n",
    "        query: in shape [b,c,h2,w2]\n",
    "        '''\n",
    "        b, t = clip.shape[:2]\n",
    "        clip = rearrange(clip, 'b t c h w -> (b t) c h w')\n",
    "\n",
    "        # get backbone features\n",
    "        if fix_backbone:\n",
    "            with torch.no_grad():\n",
    "                query_feat = self.extract_feature(query)\n",
    "                clip_feat = self.extract_feature(clip)\n",
    "        else:\n",
    "            query_feat = self.extract_feature(query)        # [b c h w]\n",
    "            clip_feat = self.extract_feature(clip)          # (b t) c h w\n",
    "        h, w = clip_feat.shape[-2:]\n",
    "\n",
    "        if torch.is_tensor(query_frame_bbox) and self.config.train.use_query_roi:\n",
    "            idx_tensor = torch.arange(b, device=clip.device).float().view(-1, 1)\n",
    "            query_frame_bbox = recover_bbox(query_frame_bbox, h, w)\n",
    "            roi_bbox = torch.cat([idx_tensor, query_frame_bbox], dim=1)\n",
    "            query_feat = torchvision.ops.roi_align(query_feat, roi_bbox, (h,w))\n",
    "\n",
    "        # reduce channel size\n",
    "        all_feat = torch.cat([query_feat, clip_feat], dim=0)\n",
    "        all_feat = self.reduce(all_feat)\n",
    "        query_feat, clip_feat = all_feat.split([b, b*t], dim=0)\n",
    "\n",
    "        if self.config.train.use_hnm and training:\n",
    "            clip_feat, query_feat = self.replicate_for_hnm(query_feat, clip_feat)   # b -> b^2\n",
    "            b = b**2\n",
    "        \n",
    "        # find spatial correspondence between query-frame\n",
    "        query_feat = rearrange(query_feat.unsqueeze(1).repeat(1,t,1,1,1), 'b t c h w -> (b t) (h w) c')  # [b*t,n,c]\n",
    "        clip_feat = rearrange(clip_feat, 'b c h w -> b (h w) c')                                         # [b*t,n,c]\n",
    "        for layer in self.CQ_corr_transformer:\n",
    "            clip_feat = layer(clip_feat, query_feat)                                                     # [b*t,n,c]\n",
    "        clip_feat = rearrange(clip_feat, 'b (h w) c -> b c h w', h=h, w=w)                               # [b*t,c,h,w]\n",
    "\n",
    "        # down-size features and find spatial-temporal correspondence\n",
    "        for head in self.down_heads:\n",
    "            clip_feat = head(clip_feat)\n",
    "            if list(clip_feat.shape[-2:]) == [self.resolution_transformer]*2:\n",
    "                clip_feat = rearrange(clip_feat, '(b t) c h w -> b (t h w) c', b=b) + self.pe_3d\n",
    "                mask = self.get_mask(clip_feat, t)\n",
    "                for layer in self.feat_corr_transformer:\n",
    "                    clip_feat = layer(clip_feat, src_mask=mask)\n",
    "                clip_feat = rearrange(clip_feat, 'b (t h w) c -> (b t) c h w', b=b, t=t, h=self.resolution_transformer, w=self.resolution_transformer)\n",
    "                break\n",
    "        \n",
    "        # refine anchors\n",
    "        anchors_xyhw = self.anchors_xyhw.to(clip_feat.device)                   # [N,4]\n",
    "        anchors_xyxy = self.anchors_xyxy.to(clip_feat.device)                   # [N,4]\n",
    "        anchors_xyhw = anchors_xyhw.reshape(1,1,-1,4)                           # [1,1,N,4]\n",
    "        anchors_xyxy = anchors_xyxy.reshape(1,1,-1,4)                           # [1,1,N,4]\n",
    "        \n",
    "        bbox_refine, prob = self.head(clip_feat)                                # [b*t,N=h*w*n*m,c]\n",
    "        bbox_refine = rearrange(bbox_refine, '(b t) N c -> b t N c', b=b, t=t)  # [b,t,N,4], in xyhw frormulation\n",
    "        prob = rearrange(prob, '(b t) N c -> b t N c', b=b, t=t)                # [b,t,N,1]\n",
    "        bbox_refine += anchors_xyhw                                             # [b,t,N,4]\n",
    "\n",
    "        center, hw = bbox_refine.split([2,2], dim=-1)                           # represented by [c_x, c_y, h, w]\n",
    "        hw = 0.5 * hw                                                           # anchor's hw is defined as real hw\n",
    "        bbox = torch.cat([center - hw, center + hw], dim=-1)                    # [b,t,N,4]\n",
    "\n",
    "        result = {\n",
    "            'center': center,           # [b,t,N,2]\n",
    "            'hw': hw,                   # [b,t,N,2]\n",
    "            'bbox': bbox,               # [b,t,N,4]\n",
    "            'prob': prob.squeeze(-1),   # [b,t,N]\n",
    "            'anchor': anchors_xyxy      # [1,1,N,4]\n",
    "        }\n",
    "        return result\n",
    "    \n",
    "\n",
    "    def get_mask(self, src, t):\n",
    "        if not torch.is_tensor(self.temporal_mask):\n",
    "            hw = src.shape[1] // t\n",
    "            thw = src.shape[1]\n",
    "            mask = torch.ones(thw, thw).float() * float('-inf')\n",
    "\n",
    "            window_size = self.window_transformer // 2\n",
    "\n",
    "            for i in range(t):\n",
    "                min_idx = max(0, (i-window_size)*hw)\n",
    "                max_idx = min(thw, (i+window_size+1)*hw)\n",
    "                mask[i*hw: (i+1)*hw, min_idx: max_idx] = 0.0\n",
    "            mask = mask.to(src.device)\n",
    "            self.temporal_mask = mask\n",
    "        return self.temporal_mask\n",
    "    \n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, in_dim=256, in_res=8, out_res=16, n=n_base_sizes, m=n_aspect_ratios):\n",
    "        super(Head, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.num_up_layers = int(math.log2(out_res // in_res))\n",
    "        self.num_layers = 3\n",
    "        \n",
    "        if self.num_up_layers > 0:\n",
    "            self.up_convs = []\n",
    "            for _ in range(self.num_up_layers):\n",
    "                self.up_convs.append(torch.nn.ConvTranspose2d(in_dim, in_dim, kernel_size=4, stride=2, padding=1))\n",
    "            self.up_convs = nn.Sequential(*self.up_convs)\n",
    "\n",
    "        self.in_conv = BasicBlock_Conv2D(in_dim=in_dim, out_dim=2*in_dim)\n",
    "\n",
    "        self.regression_conv = []\n",
    "        for i in range(self.num_layers):\n",
    "            self.regression_conv.append(BasicBlock_Conv2D(in_dim, in_dim))\n",
    "        self.regression_conv = nn.Sequential(*self.regression_conv)\n",
    "\n",
    "        self.classification_conv = []\n",
    "        for i in range(self.num_layers):\n",
    "            self.classification_conv.append(BasicBlock_Conv2D(in_dim, in_dim))\n",
    "        self.classification_conv = nn.Sequential(*self.classification_conv)\n",
    "\n",
    "        self.droupout_feat = torch.nn.Dropout(p=0.2)\n",
    "        self.droupout_cls = torch.nn.Dropout(p=0.2)\n",
    "\n",
    "        self.regression_head = nn.Conv2d(in_dim, n * m * 4, kernel_size=3, padding=1)\n",
    "        self.classification_head = nn.Conv2d(in_dim, n * m * 1, kernel_size=3, padding=1)\n",
    "\n",
    "        self.regression_head.apply(self.init_weights_conv)\n",
    "        self.classification_head.apply(self.init_weights_conv)\n",
    "\n",
    "    def init_weights_conv(self, m):\n",
    "        if type(m) == nn.Conv2d:\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=1e-6)\n",
    "            nn.init.normal_(m.bias, mean=0.0, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x in shape [B,c,h=8,w=8]\n",
    "        '''\n",
    "        if self.num_up_layers > 0:\n",
    "            x = self.up_convs(x)     # [B,c,h=16,w=16]\n",
    "\n",
    "        B, c, h, w = x.shape\n",
    "\n",
    "        feat_reg, feat_cls = self.in_conv(x).split([c, c], dim=1)   # both [B,c,h,w]\n",
    "        # dpout pos 1, seems better\n",
    "        feat_reg = self.droupout_feat(feat_reg)\n",
    "        feat_cls = self.droupout_cls(feat_cls)\n",
    "\n",
    "        feat_reg = self.regression_conv(feat_reg)        # [B,n*m*4,h,w]\n",
    "        feat_cls = self.classification_conv(feat_cls)    # [B,n*m*1,h,w]\n",
    "\n",
    "        # dpout pos 2\n",
    "\n",
    "        out_reg = self.regression_head(feat_reg)\n",
    "        out_cls = self.classification_head(feat_cls)\n",
    "\n",
    "        out_reg = rearrange(out_reg, 'B (n m c) h w -> B (h w n m) c', h=h, w=w, n=self.n, m=self.m, c=4)\n",
    "        out_cls = rearrange(out_cls, 'B (n m c) h w -> B (h w n m) c', h=h, w=w, n=self.n, m=self.m, c=1)\n",
    "\n",
    "        return out_reg, out_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T11:06:01.845216Z",
     "iopub.status.busy": "2025-11-05T11:06:01.844916Z",
     "iopub.status.idle": "2025-11-05T11:06:01.877492Z",
     "shell.execute_reply": "2025-11-05T11:06:01.876938Z",
     "shell.execute_reply.started": "2025-11-05T11:06:01.845192Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import numpy as np\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "config = edict()\n",
    "\n",
    "# experiment config\n",
    "config.exp_name = 'vq2d'\n",
    "config.exp_group = 'baseline'\n",
    "config.output_dir = './output/'\n",
    "config.log_dir = './log'\n",
    "config.workers = 8\n",
    "config.print_freq = 100\n",
    "config.vis_freq = 300\n",
    "config.eval_vis_freq = 20\n",
    "config.seed = 42\n",
    "config.inference_cache_path = ''\n",
    "config.debug = False\n",
    "\n",
    "# dataset config\n",
    "config.dataset = edict()\n",
    "config.dataset.name = 'ego4d_vq2d'\n",
    "config.dataset.name_val = 'ego4d_vq2d'\n",
    "config.dataset.query_size = 448\n",
    "config.dataset.clip_size_fine = 448\n",
    "config.dataset.clip_size_coarse = 448\n",
    "config.dataset.clip_num_frames = 30\n",
    "config.dataset.clip_num_frames_val = 30\n",
    "config.dataset.clip_sampling = 'rand'\n",
    "config.dataset.clip_reader = 'decord_balance'\n",
    "config.dataset.clip_reader_val = 'decord_balance'\n",
    "config.dataset.frame_interval = 5\n",
    "config.dataset.query_padding = False\n",
    "config.dataset.query_square = False\n",
    "config.dataset.padding_value = 'zero'\n",
    "\n",
    "# model config\n",
    "config.model = edict()\n",
    "config.model.backbone_name = 'dinov2'\n",
    "config.model.backbone_type = 'vits14'\n",
    "config.model.bakcbone_use_mae_weight = False\n",
    "config.model.fix_backbone = True\n",
    "config.model.num_transformer = 3\n",
    "config.model.type_transformer = 'global'\n",
    "config.model.resolution_transformer = 8\n",
    "config.model.resolution_anchor_feat = 16\n",
    "config.model.pe_transformer = 'sinusoidal'\n",
    "config.model.window_transformer = 10\n",
    "config.model.positive_threshold = 0.2\n",
    "config.model.positive_topk = 5\n",
    "config.model.cpt_path = '/kaggle/input/vqloc/pytorch/default/1/cpt_best_prob.pth.tar'\n",
    "\n",
    "config.train = edict()\n",
    "config.train.resume = False\n",
    "config.train.batch_size = 4\n",
    "config.train.total_iteration = 50000\n",
    "config.train.lr = 0.001\n",
    "config.train.weight_decay = 0.0001\n",
    "config.train.schedular_warmup_iter = 1000\n",
    "config.train.schedualr_milestones = [15000, 30000, 45000]\n",
    "config.train.schedular_gamma = 0.3\n",
    "config.train.grad_max = 20.0\n",
    "config.train.accumulation_step = 1\n",
    "config.train.aug_clip = True\n",
    "config.train.aug_query = True\n",
    "config.train.aug_clip_iter = 10000\n",
    "config.train.aug_brightness = 0.2\n",
    "config.train.aug_contrast = 0.2\n",
    "config.train.aug_saturation = 0.2\n",
    "config.train.aug_crop_scale = 0.8\n",
    "config.train.aug_crop_ratio_min = 0.8\n",
    "config.train.aug_crop_ratio_max = 1.2\n",
    "config.train.aug_affine_degree = 90\n",
    "config.train.aug_affine_translate = 0.1\n",
    "config.train.aug_affine_scale_min = 0.9\n",
    "config.train.aug_affine_scale_max = 1.1\n",
    "config.train.aug_affine_shear_min = -15.0\n",
    "config.train.aug_affine_shear_max = 15.0\n",
    "config.train.aug_prob_color = 0.2\n",
    "config.train.aug_prob_flip = 0.2\n",
    "config.train.aug_prob_crop = 0.2\n",
    "config.train.aug_prob_affine = 0.2\n",
    "config.train.use_hnm = False\n",
    "config.train.use_query_roi = False\n",
    "config.train.use_hnm = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T11:06:01.878486Z",
     "iopub.status.busy": "2025-11-05T11:06:01.878241Z",
     "iopub.status.idle": "2025-11-05T11:06:01.961861Z",
     "shell.execute_reply": "2025-11-05T11:06:01.961095Z",
     "shell.execute_reply.started": "2025-11-05T11:06:01.878465Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T11:06:01.964868Z",
     "iopub.status.busy": "2025-11-05T11:06:01.964616Z",
     "iopub.status.idle": "2025-11-05T11:06:04.766715Z",
     "shell.execute_reply": "2025-11-05T11:06:04.766067Z",
     "shell.execute_reply.started": "2025-11-05T11:06:01.964850Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
      "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vits14_pretrain.pth\n",
      "100%|| 84.2M/84.2M [00:00<00:00, 380MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = ClipMatcher(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T11:06:04.768035Z",
     "iopub.status.busy": "2025-11-05T11:06:04.767798Z",
     "iopub.status.idle": "2025-11-05T11:06:04.773288Z",
     "shell.execute_reply": "2025-11-05T11:06:04.772421Z",
     "shell.execute_reply.started": "2025-11-05T11:06:04.768014Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with 43938364 parameters\n"
     ]
    }
   ],
   "source": [
    "print('Model with {} parameters'.format(sum(p.numel() for p in model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T11:06:04.774153Z",
     "iopub.status.busy": "2025-11-05T11:06:04.773975Z",
     "iopub.status.idle": "2025-11-05T11:06:04.809922Z",
     "shell.execute_reply": "2025-11-05T11:06:04.809198Z",
     "shell.execute_reply.started": "2025-11-05T11:06:04.774139Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T11:06:04.810872Z",
     "iopub.status.busy": "2025-11-05T11:06:04.810660Z",
     "iopub.status.idle": "2025-11-05T11:06:04.831109Z",
     "shell.execute_reply": "2025-11-05T11:06:04.830390Z",
     "shell.execute_reply.started": "2025-11-05T11:06:04.810854Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "ClipMatcher                                                  491,520\n",
       "DinoVisionTransformer: 1-1                                 526,848\n",
       "    PatchEmbed: 2-1                                       --\n",
       "        Conv2d: 3-1                                      226,176\n",
       "        Identity: 3-2                                    --\n",
       "    ModuleList: 2-2                                       --\n",
       "        NestedTensorBlock: 3-3                           1,775,232\n",
       "        NestedTensorBlock: 3-4                           1,775,232\n",
       "        NestedTensorBlock: 3-5                           1,775,232\n",
       "        NestedTensorBlock: 3-6                           1,775,232\n",
       "        NestedTensorBlock: 3-7                           1,775,232\n",
       "        NestedTensorBlock: 3-8                           1,775,232\n",
       "        NestedTensorBlock: 3-9                           1,775,232\n",
       "        NestedTensorBlock: 3-10                          1,775,232\n",
       "        NestedTensorBlock: 3-11                          1,775,232\n",
       "        NestedTensorBlock: 3-12                          1,775,232\n",
       "        NestedTensorBlock: 3-13                          1,775,232\n",
       "        NestedTensorBlock: 3-14                          1,775,232\n",
       "    LayerNorm: 2-3                                        768\n",
       "    Identity: 2-4                                         --\n",
       "ModuleList: 1-2                                            --\n",
       "    Sequential: 2-5                                       --\n",
       "        Conv2d: 3-15                                     1,327,488\n",
       "        BatchNorm2d: 3-16                                768\n",
       "        LeakyReLU: 3-17                                  --\n",
       "    Sequential: 2-6                                       --\n",
       "        Conv2d: 3-18                                     1,327,488\n",
       "        BatchNorm2d: 3-19                                768\n",
       "        LeakyReLU: 3-20                                  --\n",
       "    Sequential: 2-7                                       --\n",
       "        Conv2d: 3-21                                     1,327,488\n",
       "        BatchNorm2d: 3-22                                768\n",
       "        LeakyReLU: 3-23                                  --\n",
       "    Sequential: 2-8                                       --\n",
       "        Conv2d: 3-24                                     1,327,488\n",
       "        BatchNorm2d: 3-25                                768\n",
       "        LeakyReLU: 3-26                                  --\n",
       "    Sequential: 2-9                                       --\n",
       "        Conv2d: 3-27                                     1,327,488\n",
       "        BatchNorm2d: 3-28                                768\n",
       "        LeakyReLU: 3-29                                  --\n",
       "Sequential: 1-3                                            --\n",
       "    Conv2d: 2-10                                          884,992\n",
       "    BatchNorm2d: 2-11                                     512\n",
       "    LeakyReLU: 2-12                                       --\n",
       "    Conv2d: 2-13                                          590,080\n",
       "    BatchNorm2d: 2-14                                     512\n",
       "    LeakyReLU: 2-15                                       --\n",
       "ModuleList: 1-4                                            --\n",
       "    TransformerDecoderLayer: 2-16                         --\n",
       "        MultiheadAttention: 3-30                         263,168\n",
       "        MultiheadAttention: 3-31                         263,168\n",
       "        Linear: 3-32                                     263,168\n",
       "        Dropout: 3-33                                    --\n",
       "        Linear: 3-34                                     262,400\n",
       "        LayerNorm: 3-35                                  512\n",
       "        LayerNorm: 3-36                                  512\n",
       "        LayerNorm: 3-37                                  512\n",
       "        Dropout: 3-38                                    --\n",
       "        Dropout: 3-39                                    --\n",
       "        Dropout: 3-40                                    --\n",
       "ModuleList: 1-5                                            --\n",
       "    Sequential: 2-17                                      --\n",
       "        Conv2d: 3-41                                     590,080\n",
       "        BatchNorm2d: 3-42                                512\n",
       "        LeakyReLU: 3-43                                  --\n",
       "    Sequential: 2-18                                      --\n",
       "        Conv2d: 3-44                                     590,080\n",
       "        BatchNorm2d: 3-45                                512\n",
       "        LeakyReLU: 3-46                                  --\n",
       "    Sequential: 2-19                                      --\n",
       "        Conv2d: 3-47                                     590,080\n",
       "        BatchNorm2d: 3-48                                512\n",
       "        LeakyReLU: 3-49                                  --\n",
       "    Sequential: 2-20                                      --\n",
       "        Conv2d: 3-50                                     590,080\n",
       "        BatchNorm2d: 3-51                                512\n",
       "        LeakyReLU: 3-52                                  --\n",
       "ModuleList: 1-6                                            --\n",
       "    TransformerEncoderLayer: 2-21                         --\n",
       "        MultiheadAttention: 3-53                         263,168\n",
       "        Linear: 3-54                                     526,336\n",
       "        Dropout: 3-55                                    --\n",
       "        Linear: 3-56                                     524,544\n",
       "        LayerNorm: 3-57                                  512\n",
       "        LayerNorm: 3-58                                  512\n",
       "        Dropout: 3-59                                    --\n",
       "        Dropout: 3-60                                    --\n",
       "    TransformerEncoderLayer: 2-22                         --\n",
       "        MultiheadAttention: 3-61                         263,168\n",
       "        Linear: 3-62                                     526,336\n",
       "        Dropout: 3-63                                    --\n",
       "        Linear: 3-64                                     524,544\n",
       "        LayerNorm: 3-65                                  512\n",
       "        LayerNorm: 3-66                                  512\n",
       "        Dropout: 3-67                                    --\n",
       "        Dropout: 3-68                                    --\n",
       "    TransformerEncoderLayer: 2-23                         --\n",
       "        MultiheadAttention: 3-69                         263,168\n",
       "        Linear: 3-70                                     526,336\n",
       "        Dropout: 3-71                                    --\n",
       "        Linear: 3-72                                     524,544\n",
       "        LayerNorm: 3-73                                  512\n",
       "        LayerNorm: 3-74                                  512\n",
       "        Dropout: 3-75                                    --\n",
       "        Dropout: 3-76                                    --\n",
       "Head: 1-7                                                  --\n",
       "    Sequential: 2-24                                      --\n",
       "        ConvTranspose2d: 3-77                            1,048,832\n",
       "    Sequential: 2-25                                      --\n",
       "        Conv2d: 3-78                                     1,180,160\n",
       "        BatchNorm2d: 3-79                                1,024\n",
       "        LeakyReLU: 3-80                                  --\n",
       "    Sequential: 2-26                                      --\n",
       "        Sequential: 3-81                                 590,592\n",
       "        Sequential: 3-82                                 590,592\n",
       "        Sequential: 3-83                                 590,592\n",
       "    Sequential: 2-27                                      --\n",
       "        Sequential: 3-84                                 590,592\n",
       "        Sequential: 3-85                                 590,592\n",
       "        Sequential: 3-86                                 590,592\n",
       "    Dropout: 2-28                                         --\n",
       "    Dropout: 2-29                                         --\n",
       "    Conv2d: 2-30                                          110,640\n",
       "    Conv2d: 2-31                                          27,660\n",
       "=====================================================================================\n",
       "Total params: 43,938,364\n",
       "Trainable params: 43,938,364\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T11:06:04.832177Z",
     "iopub.status.busy": "2025-11-05T11:06:04.831920Z",
     "iopub.status.idle": "2025-11-05T11:06:11.191860Z",
     "shell.execute_reply": "2025-11-05T11:06:11.189164Z",
     "shell.execute_reply.started": "2025-11-05T11:06:04.832160Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ClipMatcher:\n\tsize mismatch for backbone.cls_token: copying a param with shape torch.Size([1, 1, 768]) from checkpoint, the shape in current model is torch.Size([1, 1, 384]).\n\tsize mismatch for backbone.pos_embed: copying a param with shape torch.Size([1, 1370, 768]) from checkpoint, the shape in current model is torch.Size([1, 1370, 384]).\n\tsize mismatch for backbone.mask_token: copying a param with shape torch.Size([1, 768]) from checkpoint, the shape in current model is torch.Size([1, 384]).\n\tsize mismatch for backbone.patch_embed.proj.weight: copying a param with shape torch.Size([768, 3, 14, 14]) from checkpoint, the shape in current model is torch.Size([384, 3, 14, 14]).\n\tsize mismatch for backbone.patch_embed.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.0.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.0.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.0.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.0.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.0.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.0.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.0.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.0.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.0.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.0.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.0.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.0.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.0.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.0.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.1.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.1.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.1.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.1.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.1.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.1.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.1.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.1.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.1.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.1.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.1.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.1.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.1.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.1.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.2.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.2.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.2.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.2.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.2.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.2.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.2.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.2.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.2.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.2.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.2.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.2.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.2.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.2.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.3.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.3.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.3.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.3.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.3.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.3.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.3.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.3.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.3.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.3.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.3.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.3.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.3.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.3.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.4.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.4.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.4.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.4.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.4.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.4.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.4.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.4.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.4.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.4.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.4.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.4.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.4.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.4.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.5.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.5.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.5.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.5.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.5.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.5.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.5.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.5.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.5.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.5.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.5.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.5.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.5.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.5.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.6.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.6.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.6.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.6.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.6.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.6.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.6.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.6.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.6.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.6.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.6.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.6.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.6.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.6.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.7.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.7.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.7.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.7.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.7.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.7.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.7.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.7.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.7.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.7.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.7.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.7.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.7.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.7.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.8.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.8.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.8.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.8.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.8.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.8.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.8.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.8.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.8.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.8.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.8.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.8.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.8.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.8.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.9.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.9.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.9.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.9.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.9.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.9.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.9.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.9.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.9.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.9.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.9.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.9.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.9.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.9.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.10.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.10.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.10.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.10.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.10.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.10.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.10.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.10.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.10.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.10.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.10.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.10.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.10.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.10.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.11.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.11.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.11.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.11.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.11.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.11.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.11.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.11.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.11.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.11.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.11.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.11.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.11.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.11.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.0.0.weight: copying a param with shape torch.Size([768, 768, 3, 3]) from checkpoint, the shape in current model is torch.Size([384, 384, 3, 3]).\n\tsize mismatch for query_down_heads.0.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.0.1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.0.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.0.1.running_mean: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.0.1.running_var: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.1.0.weight: copying a param with shape torch.Size([768, 768, 3, 3]) from checkpoint, the shape in current model is torch.Size([384, 384, 3, 3]).\n\tsize mismatch for query_down_heads.1.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.1.1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.1.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.1.1.running_mean: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.1.1.running_var: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.2.0.weight: copying a param with shape torch.Size([768, 768, 3, 3]) from checkpoint, the shape in current model is torch.Size([384, 384, 3, 3]).\n\tsize mismatch for query_down_heads.2.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.2.1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.2.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.2.1.running_mean: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.2.1.running_var: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.3.0.weight: copying a param with shape torch.Size([768, 768, 3, 3]) from checkpoint, the shape in current model is torch.Size([384, 384, 3, 3]).\n\tsize mismatch for query_down_heads.3.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.3.1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.3.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.3.1.running_mean: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.3.1.running_var: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.4.0.weight: copying a param with shape torch.Size([768, 768, 3, 3]) from checkpoint, the shape in current model is torch.Size([384, 384, 3, 3]).\n\tsize mismatch for query_down_heads.4.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.4.1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.4.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.4.1.running_mean: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.4.1.running_var: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for reduce.0.weight: copying a param with shape torch.Size([256, 768, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 384, 3, 3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37/3634609295.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ClipMatcher:\n\tsize mismatch for backbone.cls_token: copying a param with shape torch.Size([1, 1, 768]) from checkpoint, the shape in current model is torch.Size([1, 1, 384]).\n\tsize mismatch for backbone.pos_embed: copying a param with shape torch.Size([1, 1370, 768]) from checkpoint, the shape in current model is torch.Size([1, 1370, 384]).\n\tsize mismatch for backbone.mask_token: copying a param with shape torch.Size([1, 768]) from checkpoint, the shape in current model is torch.Size([1, 384]).\n\tsize mismatch for backbone.patch_embed.proj.weight: copying a param with shape torch.Size([768, 3, 14, 14]) from checkpoint, the shape in current model is torch.Size([384, 3, 14, 14]).\n\tsize mismatch for backbone.patch_embed.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.0.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.0.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.0.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.0.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.0.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.0.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.0.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.0.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.0.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.0.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.0.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.0.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.0.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.0.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.1.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.1.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.1.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.1.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.1.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.1.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.1.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.1.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.1.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.1.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.1.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.1.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.1.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.1.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.2.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.2.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.2.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.2.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.2.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.2.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.2.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.2.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.2.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.2.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.2.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.2.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.2.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.2.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.3.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.3.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.3.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.3.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.3.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.3.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.3.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.3.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.3.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.3.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.3.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.3.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.3.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.3.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.4.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.4.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.4.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.4.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.4.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.4.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.4.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.4.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.4.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.4.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.4.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.4.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.4.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.4.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.5.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.5.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.5.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.5.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.5.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.5.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.5.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.5.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.5.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.5.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.5.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.5.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.5.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.5.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.6.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.6.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.6.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.6.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.6.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.6.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.6.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.6.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.6.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.6.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.6.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.6.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.6.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.6.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.7.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.7.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.7.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.7.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.7.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.7.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.7.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.7.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.7.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.7.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.7.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.7.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.7.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.7.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.8.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.8.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.8.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.8.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.8.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.8.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.8.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.8.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.8.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.8.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.8.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.8.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.8.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.8.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.9.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.9.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.9.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.9.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.9.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.9.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.9.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.9.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.9.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.9.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.9.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.9.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.9.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.9.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.10.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.10.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.10.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.10.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.10.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.10.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.10.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.10.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.10.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.10.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.10.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.10.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.10.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.10.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.11.norm1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.11.norm1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.11.attn.qkv.weight: copying a param with shape torch.Size([2304, 768]) from checkpoint, the shape in current model is torch.Size([1152, 384]).\n\tsize mismatch for backbone.blocks.11.attn.qkv.bias: copying a param with shape torch.Size([2304]) from checkpoint, the shape in current model is torch.Size([1152]).\n\tsize mismatch for backbone.blocks.11.attn.proj.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([384, 384]).\n\tsize mismatch for backbone.blocks.11.attn.proj.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.11.ls1.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.11.norm2.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.11.norm2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.11.mlp.fc1.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([1536, 384]).\n\tsize mismatch for backbone.blocks.11.mlp.fc1.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([1536]).\n\tsize mismatch for backbone.blocks.11.mlp.fc2.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([384, 1536]).\n\tsize mismatch for backbone.blocks.11.mlp.fc2.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.blocks.11.ls2.gamma: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.norm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for backbone.norm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.0.0.weight: copying a param with shape torch.Size([768, 768, 3, 3]) from checkpoint, the shape in current model is torch.Size([384, 384, 3, 3]).\n\tsize mismatch for query_down_heads.0.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.0.1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.0.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.0.1.running_mean: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.0.1.running_var: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.1.0.weight: copying a param with shape torch.Size([768, 768, 3, 3]) from checkpoint, the shape in current model is torch.Size([384, 384, 3, 3]).\n\tsize mismatch for query_down_heads.1.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.1.1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.1.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.1.1.running_mean: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.1.1.running_var: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.2.0.weight: copying a param with shape torch.Size([768, 768, 3, 3]) from checkpoint, the shape in current model is torch.Size([384, 384, 3, 3]).\n\tsize mismatch for query_down_heads.2.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.2.1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.2.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.2.1.running_mean: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.2.1.running_var: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.3.0.weight: copying a param with shape torch.Size([768, 768, 3, 3]) from checkpoint, the shape in current model is torch.Size([384, 384, 3, 3]).\n\tsize mismatch for query_down_heads.3.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.3.1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.3.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.3.1.running_mean: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.3.1.running_var: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.4.0.weight: copying a param with shape torch.Size([768, 768, 3, 3]) from checkpoint, the shape in current model is torch.Size([384, 384, 3, 3]).\n\tsize mismatch for query_down_heads.4.0.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.4.1.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.4.1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.4.1.running_mean: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for query_down_heads.4.1.running_var: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([384]).\n\tsize mismatch for reduce.0.weight: copying a param with shape torch.Size([256, 768, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 384, 3, 3])."
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(config.model.cpt_path, map_location='cpu')\n",
    "model.load_state_dict(checkpoint[\"state_dict\"], strict=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-05T11:06:11.192345Z",
     "iopub.status.idle": "2025-11-05T11:06:11.192613Z",
     "shell.execute_reply": "2025-11-05T11:06:11.192460Z",
     "shell.execute_reply.started": "2025-11-05T11:06:11.192451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import decord\n",
    "from torchvision import transforms as T\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.io as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-05T11:06:11.194037Z",
     "iopub.status.idle": "2025-11-05T11:06:11.194314Z",
     "shell.execute_reply": "2025-11-05T11:06:11.194212Z",
     "shell.execute_reply.started": "2025-11-05T11:06:11.194196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalize_bbox(bbox, h, w):\n",
    "    '''\n",
    "    bbox torch tensor in shape [4] or [...,4], under torch axis\n",
    "    '''\n",
    "    bbox_cp = bbox.clone()\n",
    "    if len(bbox.shape) > 1: # [N,4]\n",
    "        bbox_cp[..., 0] /= h\n",
    "        bbox_cp[..., 1] /= w\n",
    "        bbox_cp[..., 2] /= h\n",
    "        bbox_cp[..., 3] /= w\n",
    "        return bbox_cp\n",
    "    else:\n",
    "        return torch.tensor([bbox_cp[0]/h, bbox_cp[1]/w, bbox_cp[2]/h, bbox_cp[3]/w])\n",
    "\n",
    "def recover_bbox(bbox, h, w):\n",
    "    '''\n",
    "    bbox torch tensor in shape [4] or [...,4], under torch axis\n",
    "    '''\n",
    "    bbox_cp = bbox.clone()\n",
    "    if len(bbox.shape) > 1: # [N,4]\n",
    "        bbox_cp[..., 0] *= h\n",
    "        bbox_cp[..., 1] *= w\n",
    "        bbox_cp[..., 2] *= h\n",
    "        bbox_cp[..., 3] *= w\n",
    "        return bbox_cp\n",
    "    else:\n",
    "        return torch.tensor([bbox_cp[0]*h, bbox_cp[1]*w, bbox_cp[2]*h, bbox_cp[3]*w])\n",
    "\n",
    "def bbox_torchTocv2(bbox):\n",
    "    '''\n",
    "    torch, idx 0/2 for height, 1/3 for width (x,y,x,y)\n",
    "    cv2: idx 0/2 for width, 1/3 for height (y,x,y,x)\n",
    "    bbox torch tensor in shape [4] or [...,4], under torch axis\n",
    "    '''\n",
    "    bbox_cp = bbox.clone()\n",
    "    if len(bbox.shape) > 1:\n",
    "        bbox_x1 = bbox_cp[...,0].unsqueeze(-1)\n",
    "        bbox_y1 = bbox_cp[...,1].unsqueeze(-1)\n",
    "        bbox_x2 = bbox_cp[...,2].unsqueeze(-1)\n",
    "        bbox_y2 = bbox_cp[...,3].unsqueeze(-1)\n",
    "        return torch.cat([bbox_y1, bbox_x1, bbox_y2, bbox_x2], dim=-1)\n",
    "    else:\n",
    "        return torch.tensor([bbox_cp[1], bbox_cp[0], bbox_cp[3], bbox_cp[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-05T11:06:11.195081Z",
     "iopub.status.idle": "2025-11-05T11:06:11.195370Z",
     "shell.execute_reply": "2025-11-05T11:06:11.195236Z",
     "shell.execute_reply.started": "2025-11-05T11:06:11.195220Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sample_frames_balance(num_frames, frame_interval, sample, sampling='rand'):\n",
    "    '''\n",
    "    sample clips with balanced negative and positive samples\n",
    "    params:\n",
    "        num_frames: total number of frames to sample\n",
    "        query_frame: query time index\n",
    "        frame_interval: frame interval, where value 1 is for no interval (consecutive frames)\n",
    "        sample: data annotations\n",
    "        sampling: only effective for frame_interval larger than 1\n",
    "    return: \n",
    "        frame_idxs: length [num_frames]\n",
    "    '''\n",
    "    required_len = (num_frames - 1) * frame_interval + 1\n",
    "    anno_valid_idx_range = sample[\"response_track_valid_range\"]\n",
    "    anno_len = anno_valid_idx_range[1] - anno_valid_idx_range[0] + 1\n",
    "    \n",
    "    if anno_len <= required_len:\n",
    "        if anno_len < required_len:\n",
    "            num_valid = anno_len // frame_interval\n",
    "        else:\n",
    "            num_valid = num_frames\n",
    "        num_invalid = num_frames - num_valid\n",
    "        if anno_valid_idx_range[1] < required_len:\n",
    "            idx_start = random.choice(range(anno_valid_idx_range[0])) if anno_valid_idx_range[0] > 0 else 0\n",
    "            idx_end = idx_start + required_len\n",
    "        else:\n",
    "            num_prior = random.choice(range(num_invalid)) if num_invalid != 0 else 0\n",
    "            num_post = num_invalid - num_prior\n",
    "            idx_start = anno_valid_idx_range[0] - frame_interval * num_prior\n",
    "            idx_end = anno_valid_idx_range[1] + frame_interval * num_post + 1\n",
    "        intervals = np.linspace(start=idx_start, stop=idx_end, num=num_frames+1).astype(int)\n",
    "        ranges = []\n",
    "        for idx, interv in enumerate(intervals[:-1]):\n",
    "            ranges.append((interv, intervals[idx + 1]))\n",
    "        if sampling == 'rand':\n",
    "            frame_idxs_pos = [random.choice(range(x[0], x[1])) for x in ranges]\n",
    "        elif sampling == 'uniform':\n",
    "            frame_idxs_pos = [(x[0] + x[1]) // 2 for x in ranges]\n",
    "    else:\n",
    "        num_addition = anno_len - required_len\n",
    "        start = random.choice(range(num_addition))\n",
    "        frame_idxs_pos = [anno_valid_idx_range[0] + start + it for it in range(num_frames)]\n",
    "    return frame_idxs_pos\n",
    "\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "\n",
    "def read_frames_decord_balance(video_path, num_frames, frame_interval, sample, sampling='rand'):\n",
    "    video_reader = decord.VideoReader(video_path, num_threads=1)\n",
    "    vlen = len(video_reader)\n",
    "    # origin_fps = int(video_reader.get_avg_fps())\n",
    "    # gt_fps = int(sample['clip_fps'])\n",
    "    # down_rate = origin_fps // gt_fps\n",
    "    # query_frame = int(sample['query_frame'])\n",
    "    frame_idxs = sample_frames_balance(num_frames, frame_interval, sample, sampling)      # downsampled fps idxs, used to get bbox annotation\n",
    "    # before_query = torch.tensor(frame_idxs) < query_frame\n",
    "    frame_idxs_origin = [min(it, vlen - 1) for it in frame_idxs]        # origin clip fps frame idxs\n",
    "    #video_reader.skip_frames(1)\n",
    "    frames = video_reader.get_batch(frame_idxs_origin)\n",
    "    frames = frames.float() / 255\n",
    "    frames = frames.permute(0, 3, 1, 2)\n",
    "    return frames, frame_idxs#, before_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-05T11:06:11.196570Z",
     "iopub.status.idle": "2025-11-05T11:06:11.196785Z",
     "shell.execute_reply": "2025-11-05T11:06:11.196696Z",
     "shell.execute_reply.started": "2025-11-05T11:06:11.196687Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VisualQuery2DDataset(Dataset):\n",
    "    def __init__(self, clip_params, query_params, data_paths, mode='train', transform=None):\n",
    "        self.clip_params = clip_params\n",
    "        self.query_params = query_params\n",
    "        self.data_paths = data_paths\n",
    "        self.reduced_data_paths = [path.split('/')[-1] for path in self.data_paths]\n",
    "        self.mode = mode\n",
    "        \n",
    "        if transform is None:\n",
    "            self.transform = T.Compose([\n",
    "                T.Resize((self.query_params['query_size'], self.query_params['query_size'])),\n",
    "                T.ToTensor()\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "        if self.clip_params['padding_value'] == 'zero':\n",
    "            self.padding_value = 0\n",
    "        elif self.clip_params['padding_value'] == 'mean':\n",
    "            self.padding_value = 0.5\n",
    "\n",
    "        if self.mode == 'train' or self.mode == 'val':\n",
    "            self.annotations_path = os.path.join(self.data_paths[0].split('/samples')[0], 'annotations/annotations.json')\n",
    "            self.annotations = self._read_annotations(self.annotations_path)\n",
    "        else:\n",
    "            self.annotations = None\n",
    "\n",
    "    def _read_annotations(self, annotation_path):\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            anno_json = json.load(f)\n",
    "        self.annotations = []\n",
    "        for video in anno_json:\n",
    "            if video['video_id'] in self.reduced_data_paths:\n",
    "                for clip_id, clip in enumerate(video['annotations']):\n",
    "                    response_track_frame_ids = []\n",
    "                    bboxes = clip['bboxes']\n",
    "                    for bbox in bboxes:\n",
    "                        response_track_frame_ids.append(int(bbox['frame']))\n",
    "                    frame_id_min = min(response_track_frame_ids)\n",
    "                    frame_id_max = max(response_track_frame_ids)\n",
    "                    curr_anno = {\n",
    "                        'video_id': video['video_id'],\n",
    "                        'clip_id': clip_id, \n",
    "                        'response_track': clip['bboxes'],\n",
    "                        'response_track_valid_range': [frame_id_min, frame_id_max],\n",
    "                        'object_title': video['video_id'].split('_')[0],\n",
    "                    }\n",
    "                    self.annotations.append(curr_anno)\n",
    "        return self.annotations\n",
    "\n",
    "    def _get_clip_bbox(self, sample, clip_idxs, clip_h, clip_w):\n",
    "        \n",
    "        clip_with_bbox, clip_bbox = [], []\n",
    "        response_track = sample['response_track']\n",
    "        clip_bbox_all = {}\n",
    "        \n",
    "        for it in response_track:\n",
    "            clip_bbox_all[int(it['frame'])] = [it['y1'], it['x1'], it['y2'], it['x2']]\n",
    "        \n",
    "        for idx in clip_idxs:\n",
    "            if int(idx) in clip_bbox_all:\n",
    "                clip_with_bbox.append(True)\n",
    "                curr_bbox = torch.tensor(clip_bbox_all[int(idx)])\n",
    "                curr_bbox_normalize = normalize_bbox(curr_bbox, clip_h, clip_w)\n",
    "                clip_bbox.append(curr_bbox_normalize)\n",
    "            else:\n",
    "                clip_with_bbox.append(False)\n",
    "                clip_bbox.append(torch.tensor([0.0, 0.0, 0.00001, 0.00001]))\n",
    "        clip_with_bbox = torch.tensor(clip_with_bbox).float()\n",
    "        clip_bbox = torch.stack(clip_bbox, dim=0)\n",
    "        return clip_with_bbox, clip_bbox\n",
    "\n",
    "    def _get_clip_path(self, data_path):\n",
    "        clip_path = glob.glob(os.path.join(data_path, '*.mp4'))[0]\n",
    "        return clip_path\n",
    "\n",
    "    def _get_query_path(self, data_path):\n",
    "        query_path = glob.glob(os.path.join(data_path, 'object_images', '*.jpg'))\n",
    "        return query_path\n",
    "    \n",
    "    def _process_clip(self, clip, clip_bbox, clip_with_bbox):\n",
    "        '''\n",
    "        clip: in [T,C,H,W]\n",
    "        bbox: in [T,4] with torch coordinate with value range [0,1] normalized\n",
    "        clip_with_bbox: in [T]\n",
    "        '''\n",
    "        target_size = self.clip_params['fine_size']\n",
    "\n",
    "        t, _, h, w = clip.shape\n",
    "        clip_bbox = recover_bbox(clip_bbox, h, w)\n",
    "\n",
    "        try:\n",
    "            fg_idxs = torch.where(clip_with_bbox)[0].numpy().tolist()\n",
    "            idx = random.choice(fg_idxs)\n",
    "            frame = (clip[idx] * 255).permute(1,2,0).numpy().astype(np.uint8)\n",
    "            frame = Image.fromarray(frame)\n",
    "            bbox = bbox_torchTocv2(clip_bbox[idx]).tolist()\n",
    "            query = frame.crop((bbox[0], bbox[1], bbox[2], bbox[3]))\n",
    "            query_size = self.query_params['query_size']\n",
    "            query = query.resize((query_size, query_size))\n",
    "            query = torch.from_numpy(np.asarray(query) / 255.0).permute(2,0,1)\n",
    "        except:\n",
    "            query = None\n",
    "\n",
    "        max_size, min_size = max(h, w), min(h, w)\n",
    "        pad_height = True if h < w else False\n",
    "        pad_size = (max_size - min_size) // 2\n",
    "        if pad_height:\n",
    "            pad_input = [0, pad_size] * 2                   # for the left, top, right and bottom borders respectively\n",
    "            clip_bbox[:,0] += (max_size - min_size) / 2.0   # in padded image size\n",
    "            clip_bbox[:,2] += (max_size - min_size) / 2.0\n",
    "        else:\n",
    "            pad_input = [pad_size, 0] * 2\n",
    "            clip_bbox[:,1] += (max_size - min_size) / 2.0\n",
    "            clip_bbox[:,3] += (max_size - min_size) / 2.0\n",
    "        \n",
    "        transform_pad = T.Pad(pad_input, fill=self.padding_value)\n",
    "        clip = transform_pad(clip)        # square image\n",
    "        h_pad, w_pad = clip.shape[-2:]\n",
    "        clip = F.interpolate(clip, size=(target_size, target_size), mode='bilinear')#.squeeze(0)\n",
    "        clip_bbox = clip_bbox / float(h_pad)                # in range [0,1]\n",
    "\n",
    "        # if self.split == 'train':\n",
    "        #     clip_bbox, clip_with_bbox = self._process_bbox(clip_bbox, clip_with_bbox)\n",
    "        return clip, clip_bbox, clip_with_bbox, query, h, w\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load and return a sample\n",
    "        sample = self.annotations[idx]\n",
    "        data_path = os.path.join('/'.join(self.data_paths[0].split('/')[:-1]), sample['video_id'])\n",
    "        clip_path = self._get_clip_path(data_path)\n",
    "        query_path = self._get_query_path(data_path)\n",
    "        # query_images = [Image.open(img_path).convert(\"RGB\") for img_path in query_path]\n",
    "        # query_images = [self.transform(img) for img in query_images]\n",
    "        # query_images = torch.stack(query_images, dim=0)\n",
    "        query_images = Image.open(query_path[0]).convert(\"RGB\")\n",
    "        query_images = self.transform(query_images)\n",
    "        \n",
    "        sample_method = self.clip_params['sampling']\n",
    "\n",
    "        clip, clip_idxs = read_frames_decord_balance(clip_path,\n",
    "                                                    self.clip_params['num_frames'],\n",
    "                                                    self.clip_params['frame_interval'],\n",
    "                                                    sample,\n",
    "                                                sampling=sample_method)\n",
    "    \n",
    "        clip_h, clip_w = clip.shape[-2], clip.shape[-1]\n",
    "        clip_with_bbox, clip_bbox = self._get_clip_bbox(sample, clip_idxs, clip_h, clip_w)\n",
    "        clip, clip_bbox, clip_with_bbox, query, clip_h, clip_w = self._process_clip(clip, clip_bbox, clip_with_bbox)\n",
    "        \n",
    "        results = {\n",
    "            'clip': clip,\n",
    "            'clip_with_bbox': clip_with_bbox,\n",
    "            'clip_bbox': clip_bbox.float(),\n",
    "            'clip_idxs': clip_idxs,\n",
    "            'query_images': query_images,\n",
    "            'clip_h': clip_h,\n",
    "            'clip_w': clip_w,\n",
    "            # 'query': query\n",
    "        }\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-05T11:06:11.197488Z",
     "iopub.status.idle": "2025-11-05T11:06:11.197758Z",
     "shell.execute_reply": "2025-11-05T11:06:11.197661Z",
     "shell.execute_reply.started": "2025-11-05T11:06:11.197645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "abs_dir = '/kaggle/input/aeroeyes/observing/train/samples'\n",
    "data_paths = glob.glob(abs_dir+'/*')\n",
    "np.random.shuffle(data_paths)\n",
    "train_paths = data_paths[:int(0.8*len(data_paths))]\n",
    "val_paths = data_paths[int(0.8*len(data_paths)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-05T11:06:11.198639Z",
     "iopub.status.idle": "2025-11-05T11:06:11.198916Z",
     "shell.execute_reply": "2025-11-05T11:06:11.198812Z",
     "shell.execute_reply.started": "2025-11-05T11:06:11.198798Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "clip_params = {\n",
    "    'num_frames': 30,\n",
    "    'frame_interval': 30,\n",
    "    'sampling': 'rand',\n",
    "    'fine_size': 448,\n",
    "    'padding_value': 'zero',\n",
    "}\n",
    "\n",
    "query_params = {\n",
    "    'query_size': 448\n",
    "}\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((query_params['query_size'], query_params['query_size'])),\n",
    "    T.RandomRotation(30),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = VisualQuery2DDataset(clip_params, query_params, train_paths, mode='train', transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-05T11:06:11.200184Z",
     "iopub.status.idle": "2025-11-05T11:06:11.200393Z",
     "shell.execute_reply": "2025-11-05T11:06:11.200308Z",
     "shell.execute_reply.started": "2025-11-05T11:06:11.200299Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-05T11:06:11.201947Z",
     "iopub.status.idle": "2025-11-05T11:06:11.202196Z",
     "shell.execute_reply": "2025-11-05T11:06:11.202096Z",
     "shell.execute_reply.started": "2025-11-05T11:06:11.202083Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_data = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-05T11:06:11.203202Z",
     "iopub.status.idle": "2025-11-05T11:06:11.203471Z",
     "shell.execute_reply": "2025-11-05T11:06:11.203357Z",
     "shell.execute_reply.started": "2025-11-05T11:06:11.203343Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Take indices of bounding boxes\n",
    "batch_size = sample_data['clip'].size(0)\n",
    "bbox_indices = [torch.nonzero(sample_data['clip_with_bbox'][b]).view(-1) for b in range(batch_size)]\n",
    "bbox_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-05T11:06:11.204643Z",
     "iopub.status.idle": "2025-11-05T11:06:11.204879Z",
     "shell.execute_reply": "2025-11-05T11:06:11.204791Z",
     "shell.execute_reply.started": "2025-11-05T11:06:11.204782Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_sample = 4\n",
    "fig, axs = plt.subplots(4, 5, figsize=(15,10))\n",
    "frame = [random.choice(bbox_indices[i]).item() if len(bbox_indices[i])>0 else 0 for i in range(num_sample)]\n",
    "\n",
    "for i in range(num_sample):\n",
    "    clip = sample_data['clip'][i, frame[i]].permute(1,2,0).numpy()\n",
    "    h, w, _ = clip.shape\n",
    "    axs[i, 0].imshow(clip)\n",
    "    axs[i, 0].axis('off')\n",
    "    axs[i, 0].set_title('Clip Frame with BBox')\n",
    "    bbox = sample_data['clip_bbox'][i, frame[i]]\n",
    "    bbox = recover_bbox(bbox, h, w)\n",
    "    rect = plt.Rectangle((bbox[1], bbox[0]), (bbox[3]-bbox[1]), (bbox[2]-bbox[0]), linewidth=1, edgecolor='r', facecolor='none')\n",
    "    axs[i, 0].add_patch(rect)\n",
    "    # Crop the image using the bounding box\n",
    "    crop_image = sample_data['clip'][i, frame[i]].permute(1,2,0).numpy()\n",
    "    y1, x1, y2, x2 = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n",
    "    crop_image = crop_image[y1:y2, x1:x2]\n",
    "    print(w, h, crop_image.shape)\n",
    "    resized_crop = cv2.resize(crop_image, (w, h))\n",
    "    axs[i, 1].imshow(resized_crop)\n",
    "    axs[i, 1].axis('off')\n",
    "    axs[i, 1].set_title('Cropped BBox from Clip')\n",
    "    axs[i, 2].imshow(sample_data['query_images'][i].permute(1,2,0).numpy())\n",
    "    axs[i, 2].axis('off')\n",
    "    # axs[i, 2].set_title('Query Image 1')\n",
    "    # axs[i, 3].imshow(sample_data['query_images'][i, 1].permute(1,2,0).numpy())\n",
    "    # axs[i, 3].axis('off')\n",
    "    # axs[i, 3].set_title('Query Image 2')\n",
    "    # axs[i, 4].imshow(sample_data['query_images'][i, 2].permute(1,2,0).numpy())\n",
    "    # axs[i, 4].axis('off')\n",
    "    # axs[i, 4].set_title('Query Image 3')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-05T11:06:11.205819Z",
     "iopub.status.idle": "2025-11-05T11:06:11.206434Z",
     "shell.execute_reply": "2025-11-05T11:06:11.206322Z",
     "shell.execute_reply.started": "2025-11-05T11:06:11.206309Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "batch = next(iter(train_dataloader))\n",
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-05T11:06:11.207022Z",
     "iopub.status.idle": "2025-11-05T11:06:11.207265Z",
     "shell.execute_reply": "2025-11-05T11:06:11.207174Z",
     "shell.execute_reply.started": "2025-11-05T11:06:11.207164Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch['clip_with_bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-05T11:06:11.208031Z",
     "iopub.status.idle": "2025-11-05T11:06:11.208329Z",
     "shell.execute_reply": "2025-11-05T11:06:11.208196Z",
     "shell.execute_reply.started": "2025-11-05T11:06:11.208183Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output = model(batch['clip'].to(device), batch['query_images'].to(device), training=False, fix_backbone=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-05T11:06:11.209339Z",
     "iopub.status.idle": "2025-11-05T11:06:11.209630Z",
     "shell.execute_reply": "2025-11-05T11:06:11.209494Z",
     "shell.execute_reply.started": "2025-11-05T11:06:11.209479Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_k_values, max_k_idxs = torch.topk(output['prob'], k=5, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-05T11:06:11.210416Z",
     "iopub.status.idle": "2025-11-05T11:06:11.210728Z",
     "shell.execute_reply": "2025-11-05T11:06:11.210592Z",
     "shell.execute_reply.started": "2025-11-05T11:06:11.210578Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_values, max_idxs = torch.max(output['prob'], dim=-1)\n",
    "# bboxes = output['bbox'][max_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-05T11:06:11.212122Z",
     "iopub.status.idle": "2025-11-05T11:06:11.212425Z",
     "shell.execute_reply": "2025-11-05T11:06:11.212261Z",
     "shell.execute_reply.started": "2025-11-05T11:06:11.212251Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(max_values)\n",
    "print(max_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-05T11:06:11.216368Z",
     "iopub.status.idle": "2025-11-05T11:06:11.216765Z",
     "shell.execute_reply": "2025-11-05T11:06:11.216618Z",
     "shell.execute_reply.started": "2025-11-05T11:06:11.216601Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_tensor = output['bbox']\n",
    "# Index tensor: [1, 30]. Values must be between 0 and 767\n",
    "batch_indices = torch.zeros_like(max_idxs)\n",
    "# Item indices (0 to 29)\n",
    "item_indices = torch.arange(30).repeat(1, 1)\n",
    "\n",
    "# 3. Apply advanced indexing\n",
    "retrieved_data_adv = data_tensor[batch_indices, item_indices, index_tensor, :]\n",
    "\n",
    "# Output Shape: [1, 30, 4]\n",
    "print(f\"Retrieved Data Shape: {retrieved_data_adv.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-05T11:06:11.218096Z",
     "iopub.status.idle": "2025-11-05T11:06:11.218429Z",
     "shell.execute_reply": "2025-11-05T11:06:11.218279Z",
     "shell.execute_reply.started": "2025-11-05T11:06:11.218263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_tensor = output['bbox']\n",
    "index_tensor = max_k_idxs\n",
    "prepared_indices = max_k_idxs.unsqueeze(-1)\n",
    "\n",
    "retrieved_data = torch.gather(data_tensor, dim=2, index=prepared_indices)\n",
    "\n",
    "# Resulting shape: [1, 30, 5, 4]\n",
    "# This is correct: (Batch, BBoxes, 5 Indices Retrieved, 4 Coordinates)\n",
    "print(f\"Data Tensor Shape: {data_tensor.shape}\")\n",
    "print(f\"Index Tensor Shape: {index_tensor.shape}\")\n",
    "print(f\"Retrieved Data Shape: {retrieved_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-05T11:06:11.219211Z",
     "iopub.status.idle": "2025-11-05T11:06:11.219567Z",
     "shell.execute_reply": "2025-11-05T11:06:11.219394Z",
     "shell.execute_reply.started": "2025-11-05T11:06:11.219380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "retrieved_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-05T11:06:11.220545Z",
     "iopub.status.idle": "2025-11-05T11:06:11.221555Z",
     "shell.execute_reply": "2025-11-05T11:06:11.221359Z",
     "shell.execute_reply.started": "2025-11-05T11:06:11.221341Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_tensor = output['bbox']\n",
    "index_tensor = max_k_idxs\n",
    "\n",
    "# 1. Create helper indices for the first two dimensions\n",
    "\n",
    "# a. Batch Index: All 0s. Shape [1, 1, 1] for broadcasting\n",
    "batch_indices = torch.zeros(1, 1, 1, dtype=torch.long)\n",
    "\n",
    "# b. Item Index (0 to 29): Shape [1, 30, 1] for broadcasting\n",
    "item_indices = torch.arange(30).view(1, 30, 1)\n",
    "\n",
    "# c. Value Index (Your [1, 30, 5] tensor)\n",
    "value_indices = index_tensor.long()\n",
    "\n",
    "# 2. Apply advanced indexing\n",
    "retrieved_data_adv = data_tensor[batch_indices, item_indices, value_indices, :]\n",
    "\n",
    "# Resulting shape: [1, 30, 5, 4]\n",
    "print(f\"Corrected Retrieved Data Shape (Advanced Indexing): {retrieved_data_adv.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-05T11:06:11.222275Z",
     "iopub.status.idle": "2025-11-05T11:06:11.222681Z",
     "shell.execute_reply": "2025-11-05T11:06:11.222470Z",
     "shell.execute_reply.started": "2025-11-05T11:06:11.222456Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_frames = 30\n",
    "\n",
    "fig, ax = plt.subplots(30, 3, figsize=(20, 100))\n",
    "\n",
    "for i in range (num_frames):\n",
    "    clip = batch['clip'][0, i].permute(1, 2, 0).cpu().numpy()\n",
    "    h, w, _ = clip.shape\n",
    "    ax[i, 0].imshow(clip)\n",
    "    ax[i, 0].axis('off')\n",
    "    if batch['clip_with_bbox'][0, i] == 1:\n",
    "        gt_bbox = batch['clip_bbox'][0, i]\n",
    "        gt_bbox = recover_bbox(gt_bbox, h, w)\n",
    "        rect = plt.Rectangle((gt_bbox[1], gt_bbox[0]), (gt_bbox[3]-gt_bbox[1]), (gt_bbox[2]-gt_bbox[0]), linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax[i, 0].add_patch(rect)\n",
    "    ax[i, 0].set_title('GT')\n",
    "    ax[i, 1].imshow(clip)\n",
    "    for j in range (5):\n",
    "        bbox = retrieved_data_adv[0, i, j]\n",
    "        bbox = recover_bbox(bbox, h, w)\n",
    "        rect = plt.Rectangle((bbox[1], bbox[0]), (bbox[3]-bbox[1]), (bbox[2]-bbox[0]), linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax[i, 1].add_patch(rect)\n",
    "    ax[i, 1].axis('off')\n",
    "    ax[i, 1].set_title('Predicted')\n",
    "    ax[i, 2].imshow(batch['query_images'][0].permute(1,2,0).numpy())\n",
    "    ax[i, 2].axis('off')\n",
    "    ax[i, 2].set_title('Query Image')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-05T11:06:11.224099Z",
     "iopub.status.idle": "2025-11-05T11:06:11.224436Z",
     "shell.execute_reply": "2025-11-05T11:06:11.224266Z",
     "shell.execute_reply.started": "2025-11-05T11:06:11.224255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch['clip'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8631332,
     "sourceId": 13585588,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 488077,
     "modelInstanceId": 472179,
     "sourceId": 627144,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "vqloc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
